{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "sys.path.append('/Users/matangrinberg/Library/CloudStorage/GoogleDrive-matan.grinberg@gmail.com/My Drive/(21-24) University of California, Berkeley/ML HEP/parametrized-classifiers/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library imports\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "\n",
    "# standard numerical library imports\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# energyflow imports\n",
    "import energyflow as ef\n",
    "from energyflow.archs import PFN\n",
    "from energyflow.utils import data_split, remap_pids, to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global plot settings\n",
    "# from matplotlib import rc\n",
    "# import matplotlib.font_manager\n",
    "# rc('font', family='serif')\n",
    "# rc('text', usetex=True)\n",
    "# rc('font', size=22) \n",
    "# rc('xtick', labelsize=15) \n",
    "# rc('ytick', labelsize=15) \n",
    "# rc('legend', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize pT and center (y, phi)\n",
    "def normalize(x):\n",
    "    mask = x[:,0] > 0\n",
    "    yphi_avg = np.average(x[mask,1:3], weights=x[mask,0], axis=0)\n",
    "    x[mask,1:3] -= yphi_avg\n",
    "    x[mask,0] /= x[:,0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X):\n",
    "    for x in X:\n",
    "        normalize(x)\n",
    "    \n",
    "    # Remap PIDs to unique values in range [0,1], error_on_unknown flag for PIDs not in dictionary\n",
    "    remap_pids(X, pid_i=3, error_on_unknown=False)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to downloaded data from Zenodo\n",
    "data_dir = '/global/home/users/mgrinberg/parametrized-classifiers/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.load(data_dir + '1D_alphaS_train.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset['X']\n",
    "Y = dataset['Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocess_data(X)\n",
    "Y = to_categorical(Y, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = data_split(X, Y, test=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1620000, 51, 7)\n",
      "(1620000, 2)\n",
      "(180000, 51, 7)\n",
      "(180000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "print(X_val.shape)\n",
    "print(Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# network architecture parameters\n",
    "Phi_sizes = (100,100,128)\n",
    "F_sizes = (100,100,100)\n",
    "\n",
    "dctr = PFN(input_dim=7, Phi_sizes=Phi_sizes, F_sizes=F_sizes, summary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_label = 'DCTR_ee_dijets_1D_alphaS'\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint('./saved_models/' + save_label + '.h5', monitor='val_loss', verbose=2, save_best_only=True, mode='min')\n",
    "CSVLogger = keras.callbacks.CSVLogger('./logs/' + save_label + '_loss.csv', append=False)\n",
    "EarlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, restore_best_weights=True)\n",
    "callbacks = [checkpoint, CSVLogger, EarlyStopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "806/810 [============================>.] - ETA: 0s - loss: 0.7315 - acc: 0.5226\n",
      "Epoch 1: val_loss improved from inf to 0.68630, saving model to ./saved_models/DCTR_ee_dijets_1D_alphaS.h5\n",
      "810/810 [==============================] - 7s 7ms/step - loss: 0.7313 - acc: 0.5227 - val_loss: 0.6863 - val_acc: 0.5454\n",
      "Epoch 2/100\n",
      "808/810 [============================>.] - ETA: 0s - loss: 0.6850 - acc: 0.5478\n",
      "Epoch 2: val_loss improved from 0.68630 to 0.68054, saving model to ./saved_models/DCTR_ee_dijets_1D_alphaS.h5\n",
      "810/810 [==============================] - 5s 7ms/step - loss: 0.6850 - acc: 0.5478 - val_loss: 0.6805 - val_acc: 0.5573\n",
      "Epoch 3/100\n",
      "808/810 [============================>.] - ETA: 0s - loss: 0.6821 - acc: 0.5546\n",
      "Epoch 3: val_loss did not improve from 0.68054\n",
      "810/810 [==============================] - 5s 6ms/step - loss: 0.6821 - acc: 0.5546 - val_loss: 0.6815 - val_acc: 0.5557\n",
      "Epoch 4/100\n",
      "810/810 [==============================] - ETA: 0s - loss: 0.6813 - acc: 0.5567\n",
      "Epoch 4: val_loss improved from 0.68054 to 0.67987, saving model to ./saved_models/DCTR_ee_dijets_1D_alphaS.h5\n",
      "810/810 [==============================] - 5s 7ms/step - loss: 0.6813 - acc: 0.5567 - val_loss: 0.6799 - val_acc: 0.5595\n",
      "Epoch 5/100\n",
      "808/810 [============================>.] - ETA: 0s - loss: 0.6808 - acc: 0.5576\n",
      "Epoch 5: val_loss improved from 0.67987 to 0.67896, saving model to ./saved_models/DCTR_ee_dijets_1D_alphaS.h5\n",
      "810/810 [==============================] - 5s 7ms/step - loss: 0.6808 - acc: 0.5576 - val_loss: 0.6790 - val_acc: 0.5616\n",
      "Epoch 6/100\n",
      "802/810 [============================>.] - ETA: 0s - loss: 0.6805 - acc: 0.5585\n",
      "Epoch 6: val_loss improved from 0.67896 to 0.67893, saving model to ./saved_models/DCTR_ee_dijets_1D_alphaS.h5\n",
      "810/810 [==============================] - 5s 7ms/step - loss: 0.6805 - acc: 0.5585 - val_loss: 0.6789 - val_acc: 0.5611\n",
      "Epoch 7/100\n",
      "808/810 [============================>.] - ETA: 0s - loss: 0.6800 - acc: 0.5586\n",
      "Epoch 7: val_loss improved from 0.67893 to 0.67793, saving model to ./saved_models/DCTR_ee_dijets_1D_alphaS.h5\n",
      "810/810 [==============================] - 5s 7ms/step - loss: 0.6800 - acc: 0.5586 - val_loss: 0.6779 - val_acc: 0.5639\n",
      "Epoch 8/100\n",
      "806/810 [============================>.] - ETA: 0s - loss: 0.6799 - acc: 0.5591\n",
      "Epoch 8: val_loss did not improve from 0.67793\n",
      "810/810 [==============================] - 5s 6ms/step - loss: 0.6799 - acc: 0.5591 - val_loss: 0.6817 - val_acc: 0.5548\n",
      "Epoch 9/100\n",
      "807/810 [============================>.] - ETA: 0s - loss: 0.6794 - acc: 0.5605\n",
      "Epoch 9: val_loss did not improve from 0.67793\n",
      "810/810 [==============================] - 5s 7ms/step - loss: 0.6794 - acc: 0.5605 - val_loss: 0.6780 - val_acc: 0.5622\n",
      "Epoch 10/100\n",
      "803/810 [============================>.] - ETA: 0s - loss: 0.6791 - acc: 0.5609\n",
      "Epoch 10: val_loss improved from 0.67793 to 0.67737, saving model to ./saved_models/DCTR_ee_dijets_1D_alphaS.h5\n",
      "810/810 [==============================] - 5s 6ms/step - loss: 0.6791 - acc: 0.5608 - val_loss: 0.6774 - val_acc: 0.5640\n",
      "Epoch 11/100\n",
      "807/810 [============================>.] - ETA: 0s - loss: 0.6789 - acc: 0.5619\n",
      "Epoch 11: val_loss did not improve from 0.67737\n",
      "810/810 [==============================] - 5s 6ms/step - loss: 0.6789 - acc: 0.5619 - val_loss: 0.6774 - val_acc: 0.5639\n",
      "Epoch 12/100\n",
      "804/810 [============================>.] - ETA: 0s - loss: 0.6786 - acc: 0.5621\n",
      "Epoch 12: val_loss did not improve from 0.67737\n",
      "810/810 [==============================] - 5s 7ms/step - loss: 0.6786 - acc: 0.5621 - val_loss: 0.6784 - val_acc: 0.5639\n",
      "Epoch 13/100\n",
      "807/810 [============================>.] - ETA: 0s - loss: 0.6785 - acc: 0.5624\n",
      "Epoch 13: val_loss improved from 0.67737 to 0.67673, saving model to ./saved_models/DCTR_ee_dijets_1D_alphaS.h5\n",
      "810/810 [==============================] - 5s 7ms/step - loss: 0.6784 - acc: 0.5624 - val_loss: 0.6767 - val_acc: 0.5658\n",
      "Epoch 14/100\n",
      "804/810 [============================>.] - ETA: 0s - loss: 0.6783 - acc: 0.5629\n",
      "Epoch 14: val_loss improved from 0.67673 to 0.67666, saving model to ./saved_models/DCTR_ee_dijets_1D_alphaS.h5\n",
      "810/810 [==============================] - 5s 7ms/step - loss: 0.6783 - acc: 0.5629 - val_loss: 0.6767 - val_acc: 0.5654\n",
      "Epoch 15/100\n",
      "803/810 [============================>.] - ETA: 0s - loss: 0.6781 - acc: 0.5633\n",
      "Epoch 15: val_loss did not improve from 0.67666\n",
      "810/810 [==============================] - 5s 7ms/step - loss: 0.6781 - acc: 0.5632 - val_loss: 0.6769 - val_acc: 0.5658\n",
      "Epoch 16/100\n",
      "808/810 [============================>.] - ETA: 0s - loss: 0.6779 - acc: 0.5631\n",
      "Epoch 16: val_loss improved from 0.67666 to 0.67632, saving model to ./saved_models/DCTR_ee_dijets_1D_alphaS.h5\n",
      "810/810 [==============================] - 5s 7ms/step - loss: 0.6779 - acc: 0.5631 - val_loss: 0.6763 - val_acc: 0.5668\n",
      "Epoch 17/100\n",
      "808/810 [============================>.] - ETA: 0s - loss: 0.6778 - acc: 0.5639\n",
      "Epoch 17: val_loss did not improve from 0.67632\n",
      "810/810 [==============================] - 5s 6ms/step - loss: 0.6778 - acc: 0.5639 - val_loss: 0.6765 - val_acc: 0.5648\n",
      "Epoch 18/100\n",
      "806/810 [============================>.] - ETA: 0s - loss: 0.6777 - acc: 0.5639\n",
      "Epoch 18: val_loss did not improve from 0.67632\n",
      "810/810 [==============================] - 5s 7ms/step - loss: 0.6777 - acc: 0.5640 - val_loss: 0.6767 - val_acc: 0.5654\n",
      "Epoch 19/100\n",
      "805/810 [============================>.] - ETA: 0s - loss: 0.6775 - acc: 0.5641\n",
      "Epoch 19: val_loss did not improve from 0.67632\n",
      "810/810 [==============================] - 5s 7ms/step - loss: 0.6775 - acc: 0.5641 - val_loss: 0.6765 - val_acc: 0.5663\n",
      "Epoch 20/100\n",
      "802/810 [============================>.] - ETA: 0s - loss: 0.6775 - acc: 0.5644\n",
      "Epoch 20: val_loss did not improve from 0.67632\n",
      "810/810 [==============================] - 5s 6ms/step - loss: 0.6775 - acc: 0.5644 - val_loss: 0.6770 - val_acc: 0.5652\n",
      "Epoch 21/100\n",
      "808/810 [============================>.] - ETA: 0s - loss: 0.6773 - acc: 0.5647\n",
      "Epoch 21: val_loss did not improve from 0.67632\n",
      "810/810 [==============================] - 5s 6ms/step - loss: 0.6773 - acc: 0.5647 - val_loss: 0.6767 - val_acc: 0.5642\n",
      "Epoch 22/100\n",
      "804/810 [============================>.] - ETA: 0s - loss: 0.6772 - acc: 0.5650\n",
      "Epoch 22: val_loss did not improve from 0.67632\n",
      "810/810 [==============================] - 5s 7ms/step - loss: 0.6772 - acc: 0.5650 - val_loss: 0.6764 - val_acc: 0.5652\n",
      "Epoch 23/100\n",
      "806/810 [============================>.] - ETA: 0s - loss: 0.6772 - acc: 0.5654\n",
      "Epoch 23: val_loss improved from 0.67632 to 0.67606, saving model to ./saved_models/DCTR_ee_dijets_1D_alphaS.h5\n",
      "810/810 [==============================] - 5s 6ms/step - loss: 0.6772 - acc: 0.5654 - val_loss: 0.6761 - val_acc: 0.5669\n",
      "Epoch 24/100\n",
      "809/810 [============================>.] - ETA: 0s - loss: 0.6770 - acc: 0.5655\n",
      "Epoch 24: val_loss did not improve from 0.67606\n",
      "810/810 [==============================] - 5s 7ms/step - loss: 0.6770 - acc: 0.5654 - val_loss: 0.6776 - val_acc: 0.5627\n",
      "Epoch 25/100\n",
      "807/810 [============================>.] - ETA: 0s - loss: 0.6769 - acc: 0.5655\n",
      "Epoch 25: val_loss did not improve from 0.67606\n",
      "810/810 [==============================] - 5s 7ms/step - loss: 0.6769 - acc: 0.5656 - val_loss: 0.6765 - val_acc: 0.5657\n",
      "Epoch 26/100\n",
      "810/810 [==============================] - ETA: 0s - loss: 0.6769 - acc: 0.5656\n",
      "Epoch 26: val_loss did not improve from 0.67606\n",
      "810/810 [==============================] - 5s 6ms/step - loss: 0.6769 - acc: 0.5656 - val_loss: 0.6763 - val_acc: 0.5673\n",
      "Epoch 27/100\n",
      "805/810 [============================>.] - ETA: 0s - loss: 0.6767 - acc: 0.5661\n",
      "Epoch 27: val_loss did not improve from 0.67606\n",
      "810/810 [==============================] - 5s 7ms/step - loss: 0.6767 - acc: 0.5661 - val_loss: 0.6761 - val_acc: 0.5672\n",
      "Epoch 28/100\n",
      "802/810 [============================>.] - ETA: 0s - loss: 0.6766 - acc: 0.5664\n",
      "Epoch 28: val_loss did not improve from 0.67606\n",
      "810/810 [==============================] - 5s 6ms/step - loss: 0.6766 - acc: 0.5665 - val_loss: 0.6765 - val_acc: 0.5654\n",
      "Epoch 29/100\n",
      "809/810 [============================>.] - ETA: 0s - loss: 0.6765 - acc: 0.5666\n",
      "Epoch 29: val_loss improved from 0.67606 to 0.67599, saving model to ./saved_models/DCTR_ee_dijets_1D_alphaS.h5\n",
      "810/810 [==============================] - 5s 7ms/step - loss: 0.6765 - acc: 0.5666 - val_loss: 0.6760 - val_acc: 0.5662\n",
      "Epoch 30/100\n",
      "807/810 [============================>.] - ETA: 0s - loss: 0.6765 - acc: 0.5668\n",
      "Epoch 30: val_loss improved from 0.67599 to 0.67586, saving model to ./saved_models/DCTR_ee_dijets_1D_alphaS.h5\n",
      "810/810 [==============================] - 5s 6ms/step - loss: 0.6765 - acc: 0.5668 - val_loss: 0.6759 - val_acc: 0.5673\n",
      "Epoch 31/100\n",
      "802/810 [============================>.] - ETA: 0s - loss: 0.6765 - acc: 0.5661\n",
      "Epoch 31: val_loss did not improve from 0.67586\n",
      "810/810 [==============================] - 5s 6ms/step - loss: 0.6765 - acc: 0.5662 - val_loss: 0.6763 - val_acc: 0.5664\n",
      "Epoch 32/100\n",
      "803/810 [============================>.] - ETA: 0s - loss: 0.6763 - acc: 0.5667\n",
      "Epoch 32: val_loss did not improve from 0.67586\n",
      "810/810 [==============================] - 5s 7ms/step - loss: 0.6763 - acc: 0.5668 - val_loss: 0.6763 - val_acc: 0.5671\n",
      "Epoch 33/100\n",
      "805/810 [============================>.] - ETA: 0s - loss: 0.6762 - acc: 0.5674\n",
      "Epoch 33: val_loss did not improve from 0.67586\n",
      "810/810 [==============================] - 5s 7ms/step - loss: 0.6762 - acc: 0.5674 - val_loss: 0.6763 - val_acc: 0.5652\n",
      "Epoch 34/100\n",
      "804/810 [============================>.] - ETA: 0s - loss: 0.6761 - acc: 0.5673\n",
      "Epoch 34: val_loss improved from 0.67586 to 0.67585, saving model to ./saved_models/DCTR_ee_dijets_1D_alphaS.h5\n",
      "810/810 [==============================] - 5s 6ms/step - loss: 0.6761 - acc: 0.5673 - val_loss: 0.6758 - val_acc: 0.5680\n",
      "Epoch 35/100\n",
      "808/810 [============================>.] - ETA: 0s - loss: 0.6761 - acc: 0.5675\n",
      "Epoch 35: val_loss did not improve from 0.67585\n",
      "810/810 [==============================] - 5s 6ms/step - loss: 0.6761 - acc: 0.5676 - val_loss: 0.6764 - val_acc: 0.5657\n",
      "Epoch 36/100\n",
      "807/810 [============================>.] - ETA: 0s - loss: 0.6760 - acc: 0.5673\n",
      "Epoch 36: val_loss did not improve from 0.67585\n",
      "810/810 [==============================] - 5s 6ms/step - loss: 0.6761 - acc: 0.5673 - val_loss: 0.6764 - val_acc: 0.5652\n",
      "Epoch 37/100\n",
      "805/810 [============================>.] - ETA: 0s - loss: 0.6759 - acc: 0.5679\n",
      "Epoch 37: val_loss did not improve from 0.67585\n",
      "810/810 [==============================] - 5s 6ms/step - loss: 0.6759 - acc: 0.5680 - val_loss: 0.6762 - val_acc: 0.5660\n",
      "Epoch 38/100\n",
      "809/810 [============================>.] - ETA: 0s - loss: 0.6758 - acc: 0.5686\n",
      "Epoch 38: val_loss did not improve from 0.67585\n",
      "810/810 [==============================] - 5s 7ms/step - loss: 0.6758 - acc: 0.5686 - val_loss: 0.6775 - val_acc: 0.5644\n",
      "Epoch 39/100\n",
      "808/810 [============================>.] - ETA: 0s - loss: 0.6757 - acc: 0.5683\n",
      "Epoch 39: val_loss did not improve from 0.67585\n",
      "810/810 [==============================] - 5s 7ms/step - loss: 0.6757 - acc: 0.5683 - val_loss: 0.6762 - val_acc: 0.5665\n",
      "Epoch 40/100\n",
      "808/810 [============================>.] - ETA: 0s - loss: 0.6756 - acc: 0.5684\n",
      "Epoch 40: val_loss did not improve from 0.67585\n",
      "810/810 [==============================] - 5s 6ms/step - loss: 0.6756 - acc: 0.5684 - val_loss: 0.6767 - val_acc: 0.5650\n",
      "Epoch 41/100\n",
      "810/810 [==============================] - ETA: 0s - loss: 0.6755 - acc: 0.5693\n",
      "Epoch 41: val_loss did not improve from 0.67585\n",
      "810/810 [==============================] - 5s 6ms/step - loss: 0.6755 - acc: 0.5693 - val_loss: 0.6762 - val_acc: 0.5660\n",
      "Epoch 42/100\n",
      "805/810 [============================>.] - ETA: 0s - loss: 0.6755 - acc: 0.5686\n",
      "Epoch 42: val_loss did not improve from 0.67585\n",
      "810/810 [==============================] - 5s 7ms/step - loss: 0.6755 - acc: 0.5686 - val_loss: 0.6762 - val_acc: 0.5663\n",
      "Epoch 43/100\n",
      "809/810 [============================>.] - ETA: 0s - loss: 0.6753 - acc: 0.5694\n",
      "Epoch 43: val_loss did not improve from 0.67585\n",
      "810/810 [==============================] - 5s 7ms/step - loss: 0.6753 - acc: 0.5694 - val_loss: 0.6764 - val_acc: 0.5661\n",
      "Epoch 44/100\n",
      "809/810 [============================>.] - ETA: 0s - loss: 0.6753 - acc: 0.5694\n",
      "Epoch 44: val_loss did not improve from 0.67585\n",
      "Restoring model weights from the end of the best epoch: 34.\n",
      "810/810 [==============================] - 5s 6ms/step - loss: 0.6753 - acc: 0.5694 - val_loss: 0.6777 - val_acc: 0.5623\n",
      "Epoch 44: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = dctr.fit(X_train, Y_train,\n",
    "                    epochs = 100,\n",
    "                    batch_size = 2000,\n",
    "                    validation_data = (X_val, Y_val),\n",
    "                    verbose = 1, \n",
    "                    callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'],     label = 'loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val loss')\n",
    "plt.legend(loc=0)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model from saved file\n",
    "dctr.model.load_weights('./saved_models/DCTR_ee_dijets_1D_alphaS.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_0 = np.load(data_dir+'test1D_default.npz')\n",
    "test_dataset_1 = np.load(data_dir+'test1D_alphaS.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define labels for legends\n",
    "label_0 = r'$\\alpha_s=0.1365$'\n",
    "\n",
    "label_1 = r'$\\alpha_s = 0.16$'\n",
    "\n",
    "pythia_text = r'\\textsc{Pythia 8}' + '\\n' + r'$e^+e^- \\to Z \\to $ dijets' +'\\n'+ r\"anti-$k_{\\mathrm{T}}$, $R=0.8$\"\n",
    "def make_legend():\n",
    "    ax = plt.gca()\n",
    "    leg = ax.legend(frameon=False)\n",
    "    leg.set_title(pythia_text, prop={'size':14})\n",
    "    leg._legend_box.align = \"left\"\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test datasets\n",
    "X0_test = preprocess_data(test_dataset_0['jet'])\n",
    "X1_test = preprocess_data(test_dataset_1['jet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted probabilities\n",
    "preds_0 = dctr.predict(X0_test, batch_size=1000)\n",
    "preds_1 = dctr.predict(X1_test, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_0 = preds_0[:,0]/preds_0[:,1]\n",
    "weights_1 = preds_1[:,0]/preds_1[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(weights_0))\n",
    "print(max(1/weights_0))\n",
    "print(max(weights_1))\n",
    "print(max(1/weights_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_val = 3\n",
    "bins = np.linspace(0, clip_val, 101)\n",
    "plt.hist(np.clip(weights_0, 0, clip_val), bins = bins)\n",
    "plt.xlabel(\"Weight\")\n",
    "plt.ylabel('Jets per bin (normalized)')\n",
    "plt.title(\"Weights \" + label_0 + r' $\\rightarrow$ ' + label_0, fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_val = 3\n",
    "bins = np.linspace(0, clip_val, 101)\n",
    "plt.hist(np.clip(weights_1, 0, clip_val), bins = bins)\n",
    "plt.xlabel(\"Weight\")\n",
    "plt.ylabel('Jets per bin (normalized)')\n",
    "plt.title(\"Weights \" + label_0 + r' $\\rightarrow$ ' + label_1, fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Validation Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define default plot styles\n",
    "plot_style_0 = {'histtype':'step', 'color':'black', 'linewidth':2, 'linestyle':'--', 'density':True}\n",
    "plot_style_1 = {'alpha':0.5, 'density':True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "bins = np.linspace(0,40,21)\n",
    "hist0 = plt.hist(test_dataset_0['multiplicity'], bins = bins, label = label_0, **plot_style_0)\n",
    "hist1 = plt.hist(test_dataset_1['multiplicity'], bins = bins, label = label_1, **plot_style_1)\n",
    "hist2 = plt.hist(test_dataset_1['multiplicity'], bins = bins, label = label_1 + ' wgt.', weights=weights_1, **plot_style_1)\n",
    "\n",
    "plt.xlabel('Multiplicity')\n",
    "plt.ylabel('Jets per bin (normalized)')\n",
    "plt.xlim([0,40])\n",
    "make_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nsubjettiness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tau21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "bins = np.linspace(0,1,31)\n",
    "hist0 = plt.hist(test_dataset_0['tau21'], bins=bins, label=label_0, **plot_style_0)\n",
    "hist1 = plt.hist(test_dataset_1['tau21'], bins=bins, label=label_1, **plot_style_1)\n",
    "hist2 = plt.hist(test_dataset_1['tau21'], bins=bins, label=label_1 +' wgt.',  weights= weights_1, **plot_style_1)\n",
    "\n",
    "plt.xlabel('tau21')\n",
    "plt.ylabel('Jets per bin (normalized)')\n",
    "make_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tau32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "bins = np.linspace(0,1,31)\n",
    "hist0 = plt.hist(test_dataset_0['tau32'], bins=bins, label=label_0, **plot_style_0)\n",
    "hist1 = plt.hist(test_dataset_1['tau32'], bins=bins, label=label_1, **plot_style_1)\n",
    "hist2 = plt.hist(test_dataset_1['tau32'], bins=bins, label=label_1 +' wgt.',  weights= weights_1, **plot_style_1)\n",
    "\n",
    "plt.xlabel('tau32')\n",
    "plt.ylabel('Jets per bin (normalized)')\n",
    "make_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ECF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N=3, $\\beta$=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "bins = np.linspace(-15,5,31)\n",
    "hist1 = plt.hist(np.log(test_dataset_0['ECF_N3_B4']), bins=bins, label=label_0, **plot_style_0)\n",
    "hist2 = plt.hist(np.log(test_dataset_1['ECF_N3_B4']), bins=bins, label=label_1, **plot_style_1)\n",
    "hist3 = plt.hist(np.log(test_dataset_1['ECF_N3_B4']), bins=bins, label=label_1 +' wgt.',  weights= weights_1, **plot_style_1)\n",
    "\n",
    "plt.xlabel('log ECF(N=3, beta=4)')\n",
    "plt.ylabel('Jets per bin (normalized)')\n",
    "make_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N=4, $\\beta$=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "bins = np.linspace(-35,5,31)\n",
    "hist1 = plt.hist(np.log(test_dataset_0['ECF_N4_B4']), bins=bins, label=label_0, **plot_style_0)\n",
    "hist2 = plt.hist(np.log(test_dataset_1['ECF_N4_B4']), bins=bins, label=label_1, **plot_style_1)\n",
    "hist3 = plt.hist(np.log(test_dataset_1['ECF_N4_B4']), bins=bins, label=label_1 +' wgt.',  weights= weights_1, **plot_style_1)\n",
    "\n",
    "plt.xlabel('log ECF(N=4, beta=4)')\n",
    "plt.ylabel('Jets per bin (normalized)')\n",
    "make_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "bins = np.linspace(0,10,11)\n",
    "hist0 = plt.hist(test_dataset_0['number_of_kaons'], bins=bins, label=label_0, **plot_style_0)\n",
    "hist1 = plt.hist(test_dataset_1['number_of_kaons'], bins=bins, label=label_1, **plot_style_1)\n",
    "hist2 = plt.hist(test_dataset_1['number_of_kaons'], bins=bins, label=label_1 +' wgt.',  weights= weights_1, **plot_style_1)\n",
    "\n",
    "plt.xlabel('Number of kaons')\n",
    "plt.ylabel('Jets per bin (normalized)')\n",
    "make_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Curve Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddParams2Input(keras.layers.Layer):\n",
    "    \"\"\" Custom layer for tuning with DCTR: \n",
    "    Arguments:\n",
    "    - n_MC_params : (int) - the number of n_MC_params that are in X_dim\n",
    "    - default_MC_params : (list of floats) - default values for each of the MC parameters\n",
    "    - trainable_MC_params : (list of booleans) - True for parameters that you want to fit, false for parameters that should be fixed at default value\n",
    "\n",
    "    Usage: \n",
    "    Let X_dim be the input dimension of each particle to a PFN model, and n_MC_params be the number of MC parameters. \n",
    "    Defines a Layer that takes in an array of dimension \n",
    "    (batch_size, padded_multiplicity, X_dim - n_MC_params)\n",
    "    This layer appends each particle by the default_MC_params and makes then trainable or non-trainable based on trainable_MC_params\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_MC_params, default_MC_params, trainable_MC_params):\n",
    "        super(AddParams2Input, self).__init__()\n",
    "        # Definitions\n",
    "        self.n_MC_params = n_MC_params\n",
    "        self.MC_params = default_MC_params\n",
    "        self.trainable_MC_params = trainable_MC_params\n",
    "\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # Convert input MC parameters to weights and make then trainable or non-trainable\n",
    "        for i in range(self.n_MC_params):\n",
    "            self.MC_params[i] = self.add_weight(name='MC_param_{}'.format(i), \n",
    "                                                shape=(1, 1),\n",
    "                                                initializer=keras.initializers.Constant(self.MC_params[i]),\n",
    "                                                trainable=self.trainable_MC_params[i])\n",
    "            \n",
    "        self.MC_params = keras.backend.tf.concat(self.MC_params, axis = -1)\n",
    "        super(AddParams2Input, self).build(input_shape)\n",
    "    \n",
    "    def call(self, input):\n",
    "        # Add MC params to each input particle (but not to the padded rows)\n",
    "        concat = tf.transpose(keras.backend.tf.where(keras.backend.abs(input[..., 0]) > 0,\n",
    "                                                 tf.transpose(self.MC_params * keras.backend.ones_like(input[..., 0:self.n_MC_params])),\n",
    "                                                tf.transpose(keras.backend.zeros_like(input[..., 0:self.n_MC_params]))))\n",
    "        return keras.backend.concatenate([input, concat], -1)\n",
    "        \n",
    "        # concat_input_and_params = keras.backend.tf.where(keras.backend.abs(input[...,0])>0,\n",
    "        #                                                  self.MC_params*keras.backend.ones_like(input[...,0:self.n_MC_params]),\n",
    "        #                                                  keras.backend.zeros_like(input[...,0:self.n_MC_params]))\n",
    "        # return keras.backend.concatenate([input, concat_input_and_params], -1)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1]+self.n_MC_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_DCTR_fit_model(DCTR_model, \n",
    "                       X_dim, \n",
    "                       n_MC_params, \n",
    "                       default_MC_params,\n",
    "                       trainable_MC_params):\n",
    "    \"\"\" \n",
    "    Get a DCTR model that trains on the input MC parameters\n",
    "    \n",
    "    Arguments:\n",
    "    - DCTR_model : a PFN model that has been trained on a to continuously interpolate over the input MC dimensions\n",
    "    - X_dim : (int) - the dimension of the input expected by DCTR_model\n",
    "    - n_MC_params : (int) - the number of n_MC_params that are in X_dim\n",
    "    - default_MC_params : (list of floats) - default values for each of the MC parameters\n",
    "    - trainable_MC_params : (list of booleans) - True for parameters that you want to fit, false for parameters that should be fixed at default value\n",
    "\n",
    "    Returns:\n",
    "    - DCTR_fit_model: a compiled model that gradient descends only on the trainable MC parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Do sanity checks on inputs\n",
    "    assert X_dim >=n_MC_params, \"X_dim must be larger than n_MC_params. X_dim includes the dimensionality of the 4-vector + number of MC parameters\"\n",
    "    assert n_MC_params == len(default_MC_params), \"Dimension mismatch between n_MC_params and number of default MC parameters given. len(default_MC_params) must equal n_MC_params\"\n",
    "    assert n_MC_params == len(trainable_MC_params), \"Dimension mismatch between n_MC_params and trainable_MC_params. len(trainable_MC_params) must equal n_MC_params.\"\n",
    "    assert np.any(trainable_MC_params), \"All parameters are set to non-trainable.\"\n",
    "    \n",
    "    # Define input to DCTR_fit_model\n",
    "    non_param_input = keras.layers.Input((None, X_dim - n_MC_params))\n",
    "\n",
    "    # Construct layer that adds trainable and non-trainable parameters to the input\n",
    "    add_params_layer = AddParams2Input(n_MC_params, default_MC_params, trainable_MC_params)\n",
    "    time_dist     = keras.layers.TimeDistributed(add_params_layer, name='tdist')(non_param_input)     \n",
    "\n",
    "    # Set all weights in DCTR_model to non-trainable\n",
    "    for layer in DCTR_model.model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    # get the graph and the weights from the DCTR_model\n",
    "    output = DCTR_model.model(inputs = time_dist)\n",
    "\n",
    "    # Define full model\n",
    "    DCTR_fit_model = fitmodel = keras.models.Model(inputs = non_param_input, outputs = output)\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(lr=1e-4)\n",
    "    \n",
    "    # Compile with loss function\n",
    "    DCTR_fit_model.compile(optimizer=optimizer, loss='categorical_crossentropy')\n",
    "    \n",
    "    return DCTR_fit_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None, 4)]         0         \n",
      "                                                                 \n",
      " tdist (TimeDistributed)     (None, None, 7)           3         \n",
      "                                                                 \n",
      " model (Functional)          (None, 2)                 57130     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 57,133\n",
      "Trainable params: 1\n",
      "Non-trainable params: 57,132\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/clusterfs/ml4hep_nvme2/bpnachman/anaconda3/envs/ml4hep2/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "dctr_fit_model = get_DCTR_fit_model(dctr, \n",
    "                       X_dim =7, \n",
    "                       n_MC_params = 3, \n",
    "                       default_MC_params   = [0.1365, 0.68, 0.217], # default params for [alpha_s, aLund, StoUD]\n",
    "                       trainable_MC_params = [True, False, False]) # Only train alpha_s\n",
    "\n",
    "dctr_fit_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'tdist/MC_param_0:0' shape=(1, 1) dtype=float32, numpy=array([[0.1365]], dtype=float32)>]\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "print(dctr_fit_model.trainable_weights)\n",
    "keras.utils.plot_model(dctr_fit_model, \"my_first_model_with_shape_info.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_MC_params(dctr_fit_model, MC_params):\n",
    "    alphaS, aLund, StoUD = MC_params\n",
    "    weights = [np.array([[alphaS]],   dtype=np.float32),\n",
    "               np.array([[aLund]],    dtype=np.float32),\n",
    "               np.array([[StoUD]], dtype=np.float32)]\n",
    "    dctr_fit_model.layers[1].set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.1365]], dtype=float32),\n",
       " array([[0.68]], dtype=float32),\n",
       " array([[0.217]], dtype=float32)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dctr_fit_model.layers[1].get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_dataset = np.load(data_dir + 'test1D_default.npz')\n",
    "unknown_dataset = np.load(data_dir + 'test1D_alphaS.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_default = preprocess_data(default_dataset['jet'][:,:,:4])\n",
    "X_unknown = preprocess_data(unknown_dataset['jet'][:,:,:4])\n",
    "Y_default = np.zeros_like(X_unknown[:,0,0])\n",
    "Y_unknown = np.ones_like(X_unknown[:,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fit = np.concatenate((X_default, X_unknown), axis = 0)\n",
    "Y_fit = np.concatenate((Y_default, Y_unknown), axis = 0)\n",
    "Y_fit = to_categorical(Y_fit, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fit, _, Y_fit, _ = data_split(X_fit, Y_fit, test=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "X_f=X_fit[0:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = [0.12,0.68,0.217] * tf.ones((n, 51, 3))\n",
    "a2 = [0.14,0.68,0.217] * tf.ones((n, 51, 3))\n",
    "a3 = [0.16,0.68,0.217] * tf.ones((n, 51, 3))\n",
    "a4 = [0.18,0.68,0.217] * tf.ones((n, 51, 3))\n",
    "b1 = tf.concat([X_f, a1], axis=-1)\n",
    "b2 = tf.concat([X_f, a2], axis=-1)\n",
    "b3 = tf.concat([X_f, a3], axis=-1)\n",
    "b4 = tf.concat([X_f, a4], axis=-1)\n",
    "preds1 = dctr.model(b1[0:n])\n",
    "preds2 = dctr.model(b2[0:n])\n",
    "preds3 = dctr.model(b3[0:n])\n",
    "preds4 = dctr.model(b4[0:n])\n",
    "loss = keras.losses.CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.7435134, shape=(), dtype=float32)\n",
      "tf.Tensor(0.68552476, shape=(), dtype=float32)\n",
      "tf.Tensor(0.68132985, shape=(), dtype=float32)\n",
      "tf.Tensor(0.7820427, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(loss(Y_fit[0:n], preds1))\n",
    "print(loss(Y_fit[0:n], preds2))\n",
    "print(loss(Y_fit[0:n], preds3))\n",
    "print(loss(Y_fit[0:n], preds4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Loss as a function of MC parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.52459985, 0.47540015]], dtype=float32)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dctr.model(X_train[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(X, Y, dctr_model, MC_params, batch_size = 1000):\n",
    "    model = get_DCTR_fit_model(dctr_model, X_dim=7, n_MC_params=3, default_MC_params=MC_params, trainable_MC_params = [True, False, False])\n",
    "    return model.evaluate(x=X, y = Y, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800/1800 [==============================] - 14s 8ms/step - loss: 0.6751\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6750536561012268"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_loss(X_fit, Y_fit, dctr, [.15, 0.68, 0.217])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800/1800 [==============================] - 14s 8ms/step - loss: 0.7975\n",
      "1800/1800 [==============================] - 14s 8ms/step - loss: 0.7840\n",
      "1800/1800 [==============================] - 14s 7ms/step - loss: 0.7694\n",
      "1800/1800 [==============================] - 14s 8ms/step - loss: 0.7538\n",
      "1800/1800 [==============================] - 14s 8ms/step - loss: 0.7375\n",
      "1800/1800 [==============================] - 14s 8ms/step - loss: 0.7220\n",
      "1800/1800 [==============================] - 14s 8ms/step - loss: 0.7091\n",
      "1800/1800 [==============================] - 15s 8ms/step - loss: 0.6980\n",
      "1800/1800 [==============================] - 13s 7ms/step - loss: 0.6878\n",
      "1800/1800 [==============================] - 14s 8ms/step - loss: 0.6802\n",
      "1800/1800 [==============================] - 13s 7ms/step - loss: 0.6751\n",
      "1800/1800 [==============================] - 14s 7ms/step - loss: 0.6724\n",
      "1800/1800 [==============================] - 13s 7ms/step - loss: 0.6720\n",
      "1800/1800 [==============================] - 14s 8ms/step - loss: 0.6732\n",
      "1800/1800 [==============================] - 14s 8ms/step - loss: 0.6755\n",
      "1800/1800 [==============================] - 14s 8ms/step - loss: 0.6786\n",
      "1800/1800 [==============================] - 15s 8ms/step - loss: 0.6823\n"
     ]
    }
   ],
   "source": [
    "alpha_loss = np.array([(a, get_loss(X_fit, Y_fit, dctr, [a, 0.68, 0.217])) for a in np.linspace(0.1,0.18, 17)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(*alpha_loss.T)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel(r'$\\alpha_s$')\n",
    "# plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.15999999])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_min_loss(losses, x0):\n",
    "    f = sp.interpolate.interp1d(losses[:,0], losses[:,1], fill_value=\"extrapolate\")\n",
    "    return sp.optimize.minimize(f, x0=x0)['x']\n",
    "\n",
    "get_min_loss(alpha_loss, x0=0.1365)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/clusterfs/ml4hep_nvme2/bpnachman/anaconda3/envs/ml4hep2/lib/python3.8/site-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/clusterfs/ml4hep_nvme2/bpnachman/anaconda3/envs/ml4hep2/lib/python3.8/site-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/clusterfs/ml4hep_nvme2/bpnachman/anaconda3/envs/ml4hep2/lib/python3.8/site-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/clusterfs/ml4hep_nvme2/bpnachman/anaconda3/envs/ml4hep2/lib/python3.8/site-packages/keras/engine/training.py\", line 893, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/clusterfs/ml4hep_nvme2/bpnachman/anaconda3/envs/ml4hep2/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\", line 539, in minimize\n        return self.apply_gradients(grads_and_vars, name=name)\n    File \"/clusterfs/ml4hep_nvme2/bpnachman/anaconda3/envs/ml4hep2/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\", line 640, in apply_gradients\n        grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)\n    File \"/clusterfs/ml4hep_nvme2/bpnachman/anaconda3/envs/ml4hep2/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/utils.py\", line 73, in filter_empty_gradients\n        raise ValueError(f\"No gradients provided for any variable: {variable}. \"\n\n    ValueError: No gradients provided for any variable: (['tdist/MC_param_0:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'tdist/MC_param_0:0' shape=(1, 1) dtype=float32>),).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdctr_fit_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_fit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_fit\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/clusterfs/ml4hep_nvme2/bpnachman/anaconda3/envs/ml4hep2/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filew4tyz2jp.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/clusterfs/ml4hep_nvme2/bpnachman/anaconda3/envs/ml4hep2/lib/python3.8/site-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/clusterfs/ml4hep_nvme2/bpnachman/anaconda3/envs/ml4hep2/lib/python3.8/site-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/clusterfs/ml4hep_nvme2/bpnachman/anaconda3/envs/ml4hep2/lib/python3.8/site-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/clusterfs/ml4hep_nvme2/bpnachman/anaconda3/envs/ml4hep2/lib/python3.8/site-packages/keras/engine/training.py\", line 893, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/clusterfs/ml4hep_nvme2/bpnachman/anaconda3/envs/ml4hep2/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\", line 539, in minimize\n        return self.apply_gradients(grads_and_vars, name=name)\n    File \"/clusterfs/ml4hep_nvme2/bpnachman/anaconda3/envs/ml4hep2/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\", line 640, in apply_gradients\n        grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)\n    File \"/clusterfs/ml4hep_nvme2/bpnachman/anaconda3/envs/ml4hep2/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/utils.py\", line 73, in filter_empty_gradients\n        raise ValueError(f\"No gradients provided for any variable: {variable}. \"\n\n    ValueError: No gradients provided for any variable: (['tdist/MC_param_0:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'tdist/MC_param_0:0' shape=(1, 1) dtype=float32>),).\n"
     ]
    }
   ],
   "source": [
    "dctr_fit_model.fit(X_fit, Y_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_weights = keras.callbacks.LambdaCallback(on_epoch_end=lambda batch, logs: print(\"alpha_s fit = \", \n",
    "                                               dctr_fit_model.get_weights()[0][0][0]))\n",
    "fit_vals = [0.1365]\n",
    "append_weights = keras.callbacks.LambdaCallback(on_epoch_end=lambda batch, logs: \n",
    "                                               fit_vals.append(dctr_fit_model.get_weights()[0][0][0]))\n",
    "\n",
    "callbacks = [print_weights, append_weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/clusterfs/ml4hep_nvme2/bpnachman/anaconda3/envs/ml4hep2/lib/python3.8/site-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/clusterfs/ml4hep_nvme2/bpnachman/anaconda3/envs/ml4hep2/lib/python3.8/site-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/clusterfs/ml4hep_nvme2/bpnachman/anaconda3/envs/ml4hep2/lib/python3.8/site-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/clusterfs/ml4hep_nvme2/bpnachman/anaconda3/envs/ml4hep2/lib/python3.8/site-packages/keras/engine/training.py\", line 893, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/clusterfs/ml4hep_nvme2/bpnachman/anaconda3/envs/ml4hep2/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\", line 539, in minimize\n        return self.apply_gradients(grads_and_vars, name=name)\n    File \"/clusterfs/ml4hep_nvme2/bpnachman/anaconda3/envs/ml4hep2/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\", line 640, in apply_gradients\n        grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)\n    File \"/clusterfs/ml4hep_nvme2/bpnachman/anaconda3/envs/ml4hep2/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/utils.py\", line 73, in filter_empty_gradients\n        raise ValueError(f\"No gradients provided for any variable: {variable}. \"\n\n    ValueError: No gradients provided for any variable: (['tdist/MC_param_0:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'tdist/MC_param_0:0' shape=(1, 1) dtype=float32>),).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdctr_fit_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_fit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_fit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/clusterfs/ml4hep_nvme2/bpnachman/anaconda3/envs/ml4hep2/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filew4tyz2jp.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/clusterfs/ml4hep_nvme2/bpnachman/anaconda3/envs/ml4hep2/lib/python3.8/site-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/clusterfs/ml4hep_nvme2/bpnachman/anaconda3/envs/ml4hep2/lib/python3.8/site-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/clusterfs/ml4hep_nvme2/bpnachman/anaconda3/envs/ml4hep2/lib/python3.8/site-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/clusterfs/ml4hep_nvme2/bpnachman/anaconda3/envs/ml4hep2/lib/python3.8/site-packages/keras/engine/training.py\", line 893, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/clusterfs/ml4hep_nvme2/bpnachman/anaconda3/envs/ml4hep2/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\", line 539, in minimize\n        return self.apply_gradients(grads_and_vars, name=name)\n    File \"/clusterfs/ml4hep_nvme2/bpnachman/anaconda3/envs/ml4hep2/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\", line 640, in apply_gradients\n        grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)\n    File \"/clusterfs/ml4hep_nvme2/bpnachman/anaconda3/envs/ml4hep2/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/utils.py\", line 73, in filter_empty_gradients\n        raise ValueError(f\"No gradients provided for any variable: {variable}. \"\n\n    ValueError: No gradients provided for any variable: (['tdist/MC_param_0:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'tdist/MC_param_0:0' shape=(1, 1) dtype=float32>),).\n"
     ]
    }
   ],
   "source": [
    "dctr_fit_model.fit(X_fit, Y_fit,\n",
    "                   epochs=10, \n",
    "                   batch_size=10000,\n",
    "                   callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fit_vals, marker='o')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(r'$\\alpha_s$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
