{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "sys.path.append('/Users/matangrinberg/Library/CloudStorage/GoogleDrive-matan.grinberg@gmail.com/My Drive/(21-24) University of California, Berkeley/ML HEP/parametrized-classifiers/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library imports\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# standard numerical library imports\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# energyflow imports\n",
    "import energyflow as ef\n",
    "from energyflow.archs import PFN\n",
    "from energyflow.utils import data_split, remap_pids, to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Global plot settings\n",
    "# from matplotlib import rc\n",
    "# import matplotlib.font_manager\n",
    "# rc('font', family='serif')\n",
    "# rc('text', usetex=True)\n",
    "# rc('font', size=22) \n",
    "# rc('xtick', labelsize=15) \n",
    "# rc('ytick', labelsize=15) \n",
    "# rc('legend', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize pT and center (y, phi)\n",
    "def normalize(x):\n",
    "    mask = np.abs(x[:,0]) > 0\n",
    "    yphi_avg = np.average(x[mask,1:3], weights=x[mask,0], axis=0)\n",
    "    x[mask,1:3] -= yphi_avg\n",
    "    x[mask,0] /= x[:,0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X):\n",
    "    for x in X:\n",
    "        normalize(x)\n",
    "    \n",
    "    # Remap PIDs to unique values in range [0,1], error_on_unknown flag for PIDs not in dictionary\n",
    "    remap_pids(X, pid_i=3, error_on_unknown=False)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to downloaded data from Zenodo\n",
    "data_dir = '/global/home/users/mgrinberg/parametrized-classifiers/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = np.load(data_dir + '3D_train.npz')\n",
    "t_list = [1, 0.1, 0.01, 0.005, 0.001, 0.0001]\n",
    "t_string = '0.001'\n",
    "# t_string = '0.05'\n",
    "# t_string = '0.01'\n",
    "# t_string = '0.005'\n",
    "# t_string = '0.001'\n",
    "# t_string = '0.0005'\n",
    "dataset1 = np.load(data_dir + 'interpolate_standard_n100000t1.npz')\n",
    "\n",
    "dataset2 = np.load(data_dir + 'interpolate_standard_n100000t' + t_string + '.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset1['arr_0']\n",
    "Y = dataset1['arr_1']\n",
    "X2 = dataset2['arr_0']\n",
    "Y2 = dataset2['arr_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 51, 7)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = preprocess_data(X)\n",
    "Y1 = to_categorical(Y, num_classes=2)\n",
    "X2 = preprocess_data(X2)\n",
    "Y2 = to_categorical(Y2, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = data_split(X1, Y1, test=0.1, shuffle=True)\n",
    "X_train2, X_val2, Y_train2, Y_val2 = data_split(X2, Y2, test=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180000, 51, 7)\n",
      "(180000, 2)\n",
      "(20000, 51, 7)\n",
      "(20000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "print(X_val.shape)\n",
    "print(Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# network architecture parameters\n",
    "Phi_sizes = (100,100, 128)\n",
    "F_sizes = (100,100, 100)\n",
    "\n",
    "dctr = PFN(input_dim=7, Phi_sizes=Phi_sizes, F_sizes=F_sizes, summary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_label = 'DCTR_ee_dijets_3D'\n",
    "save_label = 'DCTR_test_t' + t_string\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint('./saved_models/' + save_label + '.h5', monitor='val_loss', verbose=2, save_best_only=True, mode='min')\n",
    "CSVLogger = keras.callbacks.CSVLogger('./logs/' + save_label + '_loss.csv', append=False)\n",
    "EarlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, restore_best_weights=True)\n",
    "callbacks = [checkpoint, CSVLogger, EarlyStopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "893/900 [============================>.] - ETA: 0s - loss: 1.1149 - acc: 0.5112\n",
      "Epoch 1: val_loss improved from inf to 0.70225, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 1.1116 - acc: 0.5113 - val_loss: 0.7022 - val_acc: 0.5190\n",
      "Epoch 2/100\n",
      "897/900 [============================>.] - ETA: 0s - loss: 0.7579 - acc: 0.5222\n",
      "Epoch 2: val_loss improved from 0.70225 to 0.68908, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.7576 - acc: 0.5223 - val_loss: 0.6891 - val_acc: 0.5377\n",
      "Epoch 3/100\n",
      "897/900 [============================>.] - ETA: 0s - loss: 0.7061 - acc: 0.5246\n",
      "Epoch 3: val_loss improved from 0.68908 to 0.68703, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.7060 - acc: 0.5247 - val_loss: 0.6870 - val_acc: 0.5169\n",
      "Epoch 4/100\n",
      "898/900 [============================>.] - ETA: 0s - loss: 0.6869 - acc: 0.5206\n",
      "Epoch 4: val_loss improved from 0.68703 to 0.68686, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6869 - acc: 0.5206 - val_loss: 0.6869 - val_acc: 0.5197\n",
      "Epoch 5/100\n",
      "888/900 [============================>.] - ETA: 0s - loss: 0.6864 - acc: 0.5250\n",
      "Epoch 5: val_loss improved from 0.68686 to 0.68559, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6864 - acc: 0.5250 - val_loss: 0.6856 - val_acc: 0.5294\n",
      "Epoch 6/100\n",
      "897/900 [============================>.] - ETA: 0s - loss: 0.6860 - acc: 0.5256\n",
      "Epoch 6: val_loss did not improve from 0.68559\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6860 - acc: 0.5256 - val_loss: 0.6870 - val_acc: 0.5185\n",
      "Epoch 7/100\n",
      "897/900 [============================>.] - ETA: 0s - loss: 0.6845 - acc: 0.5319\n",
      "Epoch 7: val_loss improved from 0.68559 to 0.68400, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6845 - acc: 0.5319 - val_loss: 0.6840 - val_acc: 0.5421\n",
      "Epoch 8/100\n",
      "893/900 [============================>.] - ETA: 0s - loss: 0.6857 - acc: 0.5243\n",
      "Epoch 8: val_loss did not improve from 0.68400\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6857 - acc: 0.5243 - val_loss: 0.6866 - val_acc: 0.5206\n",
      "Epoch 9/100\n",
      "895/900 [============================>.] - ETA: 0s - loss: 0.6855 - acc: 0.5238\n",
      "Epoch 9: val_loss did not improve from 0.68400\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6855 - acc: 0.5237 - val_loss: 0.6862 - val_acc: 0.5242\n",
      "Epoch 10/100\n",
      "897/900 [============================>.] - ETA: 0s - loss: 0.6856 - acc: 0.5193\n",
      "Epoch 10: val_loss did not improve from 0.68400\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6856 - acc: 0.5193 - val_loss: 0.6851 - val_acc: 0.5196\n",
      "Epoch 11/100\n",
      "897/900 [============================>.] - ETA: 0s - loss: 0.6828 - acc: 0.5348\n",
      "Epoch 11: val_loss improved from 0.68400 to 0.67859, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6828 - acc: 0.5349 - val_loss: 0.6786 - val_acc: 0.5503\n",
      "Epoch 12/100\n",
      "900/900 [==============================] - ETA: 0s - loss: 0.6833 - acc: 0.5295\n",
      "Epoch 12: val_loss improved from 0.67859 to 0.67734, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6833 - acc: 0.5295 - val_loss: 0.6773 - val_acc: 0.5470\n",
      "Epoch 13/100\n",
      "895/900 [============================>.] - ETA: 0s - loss: 0.6841 - acc: 0.5284\n",
      "Epoch 13: val_loss did not improve from 0.67734\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.6841 - acc: 0.5283 - val_loss: 0.6862 - val_acc: 0.5196\n",
      "Epoch 14/100\n",
      "896/900 [============================>.] - ETA: 0s - loss: 0.6839 - acc: 0.5237\n",
      "Epoch 14: val_loss did not improve from 0.67734\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6839 - acc: 0.5237 - val_loss: 0.6849 - val_acc: 0.5284\n",
      "Epoch 15/100\n",
      "891/900 [============================>.] - ETA: 0s - loss: 0.6801 - acc: 0.5375\n",
      "Epoch 15: val_loss improved from 0.67734 to 0.67501, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6801 - acc: 0.5376 - val_loss: 0.6750 - val_acc: 0.5492\n",
      "Epoch 16/100\n",
      "900/900 [==============================] - ETA: 0s - loss: 0.6761 - acc: 0.5469\n",
      "Epoch 16: val_loss improved from 0.67501 to 0.67371, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6761 - acc: 0.5469 - val_loss: 0.6737 - val_acc: 0.5466\n",
      "Epoch 17/100\n",
      "897/900 [============================>.] - ETA: 0s - loss: 0.6746 - acc: 0.5487\n",
      "Epoch 17: val_loss did not improve from 0.67371\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6746 - acc: 0.5486 - val_loss: 0.6761 - val_acc: 0.5474\n",
      "Epoch 18/100\n",
      "894/900 [============================>.] - ETA: 0s - loss: 0.6741 - acc: 0.5487\n",
      "Epoch 18: val_loss improved from 0.67371 to 0.67254, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.6741 - acc: 0.5486 - val_loss: 0.6725 - val_acc: 0.5475\n",
      "Epoch 19/100\n",
      "891/900 [============================>.] - ETA: 0s - loss: 0.6728 - acc: 0.5487\n",
      "Epoch 19: val_loss did not improve from 0.67254\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6728 - acc: 0.5489 - val_loss: 0.6733 - val_acc: 0.5500\n",
      "Epoch 20/100\n",
      "898/900 [============================>.] - ETA: 0s - loss: 0.6711 - acc: 0.5509\n",
      "Epoch 20: val_loss improved from 0.67254 to 0.67138, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6711 - acc: 0.5509 - val_loss: 0.6714 - val_acc: 0.5502\n",
      "Epoch 21/100\n",
      "894/900 [============================>.] - ETA: 0s - loss: 0.6702 - acc: 0.5516\n",
      "Epoch 21: val_loss did not improve from 0.67138\n",
      "900/900 [==============================] - 3s 3ms/step - loss: 0.6702 - acc: 0.5516 - val_loss: 0.6786 - val_acc: 0.5508\n",
      "Epoch 22/100\n",
      "887/900 [============================>.] - ETA: 0s - loss: 0.6695 - acc: 0.5523\n",
      "Epoch 22: val_loss improved from 0.67138 to 0.67056, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6695 - acc: 0.5525 - val_loss: 0.6706 - val_acc: 0.5534\n",
      "Epoch 23/100\n",
      "897/900 [============================>.] - ETA: 0s - loss: 0.6683 - acc: 0.5538\n",
      "Epoch 23: val_loss did not improve from 0.67056\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6683 - acc: 0.5538 - val_loss: 0.6727 - val_acc: 0.5501\n",
      "Epoch 24/100\n",
      "895/900 [============================>.] - ETA: 0s - loss: 0.6691 - acc: 0.5544\n",
      "Epoch 24: val_loss did not improve from 0.67056\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6691 - acc: 0.5544 - val_loss: 0.6745 - val_acc: 0.5503\n",
      "Epoch 25/100\n",
      "896/900 [============================>.] - ETA: 0s - loss: 0.6680 - acc: 0.5570\n",
      "Epoch 25: val_loss improved from 0.67056 to 0.66890, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6680 - acc: 0.5568 - val_loss: 0.6689 - val_acc: 0.5487\n",
      "Epoch 26/100\n",
      "891/900 [============================>.] - ETA: 0s - loss: 0.6662 - acc: 0.5577\n",
      "Epoch 26: val_loss improved from 0.66890 to 0.66837, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6662 - acc: 0.5580 - val_loss: 0.6684 - val_acc: 0.5546\n",
      "Epoch 27/100\n",
      "896/900 [============================>.] - ETA: 0s - loss: 0.6644 - acc: 0.5595\n",
      "Epoch 27: val_loss did not improve from 0.66837\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6644 - acc: 0.5593 - val_loss: 0.6687 - val_acc: 0.5547\n",
      "Epoch 28/100\n",
      "887/900 [============================>.] - ETA: 0s - loss: 0.6645 - acc: 0.5587\n",
      "Epoch 28: val_loss improved from 0.66837 to 0.66777, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6644 - acc: 0.5586 - val_loss: 0.6678 - val_acc: 0.5524\n",
      "Epoch 29/100\n",
      "897/900 [============================>.] - ETA: 0s - loss: 0.6626 - acc: 0.5608\n",
      "Epoch 29: val_loss improved from 0.66777 to 0.66485, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6626 - acc: 0.5608 - val_loss: 0.6649 - val_acc: 0.5587\n",
      "Epoch 30/100\n",
      "900/900 [==============================] - ETA: 0s - loss: 0.6616 - acc: 0.5636\n",
      "Epoch 30: val_loss did not improve from 0.66485\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.6616 - acc: 0.5636 - val_loss: 0.6662 - val_acc: 0.5532\n",
      "Epoch 31/100\n",
      "898/900 [============================>.] - ETA: 0s - loss: 0.6598 - acc: 0.5652\n",
      "Epoch 31: val_loss did not improve from 0.66485\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6598 - acc: 0.5652 - val_loss: 0.6682 - val_acc: 0.5561\n",
      "Epoch 32/100\n",
      "895/900 [============================>.] - ETA: 0s - loss: 0.6589 - acc: 0.5659\n",
      "Epoch 32: val_loss improved from 0.66485 to 0.66268, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6589 - acc: 0.5659 - val_loss: 0.6627 - val_acc: 0.5637\n",
      "Epoch 33/100\n",
      "894/900 [============================>.] - ETA: 0s - loss: 0.6578 - acc: 0.5673\n",
      "Epoch 33: val_loss did not improve from 0.66268\n",
      "900/900 [==============================] - 3s 3ms/step - loss: 0.6579 - acc: 0.5674 - val_loss: 0.6647 - val_acc: 0.5587\n",
      "Epoch 34/100\n",
      "898/900 [============================>.] - ETA: 0s - loss: 0.6585 - acc: 0.5657\n",
      "Epoch 34: val_loss did not improve from 0.66268\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6585 - acc: 0.5657 - val_loss: 0.6647 - val_acc: 0.5644\n",
      "Epoch 35/100\n",
      "887/900 [============================>.] - ETA: 0s - loss: 0.6564 - acc: 0.5686\n",
      "Epoch 35: val_loss improved from 0.66268 to 0.66182, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6564 - acc: 0.5687 - val_loss: 0.6618 - val_acc: 0.5656\n",
      "Epoch 36/100\n",
      "897/900 [============================>.] - ETA: 0s - loss: 0.6547 - acc: 0.5721\n",
      "Epoch 36: val_loss did not improve from 0.66182\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6547 - acc: 0.5719 - val_loss: 0.6622 - val_acc: 0.5652\n",
      "Epoch 37/100\n",
      "893/900 [============================>.] - ETA: 0s - loss: 0.6539 - acc: 0.5718\n",
      "Epoch 37: val_loss did not improve from 0.66182\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6539 - acc: 0.5719 - val_loss: 0.6662 - val_acc: 0.5556\n",
      "Epoch 38/100\n",
      "896/900 [============================>.] - ETA: 0s - loss: 0.6545 - acc: 0.5698\n",
      "Epoch 38: val_loss did not improve from 0.66182\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6545 - acc: 0.5697 - val_loss: 0.6620 - val_acc: 0.5648\n",
      "Epoch 39/100\n",
      "898/900 [============================>.] - ETA: 0s - loss: 0.6518 - acc: 0.5731\n",
      "Epoch 39: val_loss improved from 0.66182 to 0.66146, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6519 - acc: 0.5730 - val_loss: 0.6615 - val_acc: 0.5618\n",
      "Epoch 40/100\n",
      "895/900 [============================>.] - ETA: 0s - loss: 0.6502 - acc: 0.5778\n",
      "Epoch 40: val_loss did not improve from 0.66146\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6501 - acc: 0.5778 - val_loss: 0.6637 - val_acc: 0.5670\n",
      "Epoch 41/100\n",
      "894/900 [============================>.] - ETA: 0s - loss: 0.6492 - acc: 0.5812\n",
      "Epoch 41: val_loss improved from 0.66146 to 0.66131, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6492 - acc: 0.5812 - val_loss: 0.6613 - val_acc: 0.5718\n",
      "Epoch 42/100\n",
      "894/900 [============================>.] - ETA: 0s - loss: 0.6484 - acc: 0.5796\n",
      "Epoch 42: val_loss did not improve from 0.66131\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.6483 - acc: 0.5796 - val_loss: 0.6633 - val_acc: 0.5645\n",
      "Epoch 43/100\n",
      "897/900 [============================>.] - ETA: 0s - loss: 0.6473 - acc: 0.5799\n",
      "Epoch 43: val_loss improved from 0.66131 to 0.66121, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6473 - acc: 0.5799 - val_loss: 0.6612 - val_acc: 0.5703\n",
      "Epoch 44/100\n",
      "887/900 [============================>.] - ETA: 0s - loss: 0.6458 - acc: 0.5829\n",
      "Epoch 44: val_loss did not improve from 0.66121\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6459 - acc: 0.5830 - val_loss: 0.6622 - val_acc: 0.5683\n",
      "Epoch 45/100\n",
      "899/900 [============================>.] - ETA: 0s - loss: 0.6451 - acc: 0.5833\n",
      "Epoch 45: val_loss did not improve from 0.66121\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6451 - acc: 0.5833 - val_loss: 0.6630 - val_acc: 0.5695\n",
      "Epoch 46/100\n",
      "898/900 [============================>.] - ETA: 0s - loss: 0.6440 - acc: 0.5841\n",
      "Epoch 46: val_loss did not improve from 0.66121\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6440 - acc: 0.5841 - val_loss: 0.6631 - val_acc: 0.5699\n",
      "Epoch 47/100\n",
      "898/900 [============================>.] - ETA: 0s - loss: 0.6428 - acc: 0.5856\n",
      "Epoch 47: val_loss did not improve from 0.66121\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6428 - acc: 0.5857 - val_loss: 0.6614 - val_acc: 0.5702\n",
      "Epoch 48/100\n",
      "898/900 [============================>.] - ETA: 0s - loss: 0.6429 - acc: 0.5853\n",
      "Epoch 48: val_loss did not improve from 0.66121\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6429 - acc: 0.5853 - val_loss: 0.6636 - val_acc: 0.5726\n",
      "Epoch 49/100\n",
      "890/900 [============================>.] - ETA: 0s - loss: 0.6409 - acc: 0.5869\n",
      "Epoch 49: val_loss improved from 0.66121 to 0.66075, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6410 - acc: 0.5868 - val_loss: 0.6608 - val_acc: 0.5716\n",
      "Epoch 50/100\n",
      "900/900 [==============================] - ETA: 0s - loss: 0.6396 - acc: 0.5877\n",
      "Epoch 50: val_loss improved from 0.66075 to 0.65954, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6396 - acc: 0.5877 - val_loss: 0.6595 - val_acc: 0.5746\n",
      "Epoch 51/100\n",
      "899/900 [============================>.] - ETA: 0s - loss: 0.6390 - acc: 0.5894\n",
      "Epoch 51: val_loss did not improve from 0.65954\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.6389 - acc: 0.5895 - val_loss: 0.6612 - val_acc: 0.5725\n",
      "Epoch 52/100\n",
      "888/900 [============================>.] - ETA: 0s - loss: 0.6374 - acc: 0.5893\n",
      "Epoch 52: val_loss did not improve from 0.65954\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6375 - acc: 0.5893 - val_loss: 0.6614 - val_acc: 0.5675\n",
      "Epoch 53/100\n",
      "895/900 [============================>.] - ETA: 0s - loss: 0.6367 - acc: 0.5916\n",
      "Epoch 53: val_loss did not improve from 0.65954\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6368 - acc: 0.5915 - val_loss: 0.6619 - val_acc: 0.5716\n",
      "Epoch 54/100\n",
      "888/900 [============================>.] - ETA: 0s - loss: 0.6362 - acc: 0.5913\n",
      "Epoch 54: val_loss did not improve from 0.65954\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6363 - acc: 0.5913 - val_loss: 0.6700 - val_acc: 0.5731\n",
      "Epoch 55/100\n",
      "897/900 [============================>.] - ETA: 0s - loss: 0.6349 - acc: 0.5935\n",
      "Epoch 55: val_loss did not improve from 0.65954\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6350 - acc: 0.5934 - val_loss: 0.6631 - val_acc: 0.5715\n",
      "Epoch 56/100\n",
      "888/900 [============================>.] - ETA: 0s - loss: 0.6342 - acc: 0.5926\n",
      "Epoch 56: val_loss did not improve from 0.65954\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6342 - acc: 0.5926 - val_loss: 0.6638 - val_acc: 0.5724\n",
      "Epoch 57/100\n",
      "900/900 [==============================] - ETA: 0s - loss: 0.6333 - acc: 0.5922\n",
      "Epoch 57: val_loss did not improve from 0.65954\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6333 - acc: 0.5922 - val_loss: 0.6609 - val_acc: 0.5765\n",
      "Epoch 58/100\n",
      "898/900 [============================>.] - ETA: 0s - loss: 0.6320 - acc: 0.5942\n",
      "Epoch 58: val_loss did not improve from 0.65954\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6320 - acc: 0.5942 - val_loss: 0.6650 - val_acc: 0.5780\n",
      "Epoch 59/100\n",
      "897/900 [============================>.] - ETA: 0s - loss: 0.6315 - acc: 0.5958\n",
      "Epoch 59: val_loss did not improve from 0.65954\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6315 - acc: 0.5958 - val_loss: 0.6629 - val_acc: 0.5752\n",
      "Epoch 60/100\n",
      "888/900 [============================>.] - ETA: 0s - loss: 0.6305 - acc: 0.5972\n",
      "Epoch 60: val_loss did not improve from 0.65954\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6305 - acc: 0.5973 - val_loss: 0.6635 - val_acc: 0.5764\n",
      "Epoch 60: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = dctr.fit(X_train2, Y_train2,\n",
    "                    epochs = 100,\n",
    "                    batch_size = 200,\n",
    "                    validation_data = (X_val, Y_val),\n",
    "                    verbose = 1, \n",
    "                    callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'],     label = 'loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val loss')\n",
    "plt.legend(loc=0)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 1s 1ms/step\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "625/625 [==============================] - 1s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# load model from saved file\n",
    "\n",
    "comp_losses = []\n",
    "for t in t_list:\n",
    "    dctr.model.load_weights('./saved_models/DCTR_test_t' + str(t) + '.h5')\n",
    "    pred = dctr.predict(X_val)\n",
    "    loss = np.sum(tf.keras.metrics.binary_crossentropy(Y_val, pred))/pred.shape[0]\n",
    "    comp_losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.635302587890625,\n",
       " 0.65094482421875,\n",
       " 0.653648779296875,\n",
       " 0.6589138671875,\n",
       " 0.659540966796875,\n",
       " 1.208744921875]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(tf.keras.metrics.binary_crossentropy(Y_val, a1))/a1.shape[0])\n",
    "print(np.sum(tf.keras.metrics.binary_crossentropy(Y_val, a2))/a2.shape[0])\n",
    "print(np.sum(tf.keras.metrics.binary_crossentropy(Y_val, a3))/a3.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([np.sum(tf.keras.metrics.binary_crossentropy(Y_val, a1))/a1.shape[0], np.sum(tf.keras.metrics.binary_crossentropy(Y_val, a2))/a2.shape[0], np.sum(tf.keras.metrics.binary_crossentropy(Y_val, a3))/a3.shape[0]])\n",
    "b = np.log(np.array([1, 0.005, 0.0001]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100000*0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(t_list, comp_losses, label='loss on uniform dataset')\n",
    "plt.legend(loc=0)\n",
    "plt.ylabel('Average BCE Loss')\n",
    "plt.xlabel('t')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_0 = np.load(data_dir+'test1D_default.npz')\n",
    "test_dataset_1 = np.load(data_dir+'test_3D_known.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define labels for legends\n",
    "label_0 = r'Default'\n",
    "\n",
    "label_1 = r'Non-default'\n",
    "\n",
    "pythia_text = r'\\textsc{Pythia 8}' + '\\n' + r'$e^+e^- \\to Z \\to $ dijets' +'\\n'+ r\"anti-$k_{\\mathrm{T}}$, $R=0.8$\"\n",
    "def make_legend():\n",
    "    ax = plt.gca()\n",
    "    leg = ax.legend(frameon=False)\n",
    "    leg.set_title(pythia_text, prop={'size':14})\n",
    "    leg._legend_box.align = \"left\"\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test datasets\n",
    "X0_test = preprocess_data(test_dataset_0['jet'])\n",
    "X1_test = preprocess_data(test_dataset_1['jet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted probabilities\n",
    "preds_0 = dctr.predict(X0_test, batch_size=1000)\n",
    "preds_1 = dctr.predict(X1_test, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_0 = preds_0[:,0]/preds_0[:,1]\n",
    "weights_1 = preds_1[:,0]/preds_1[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(weights_0))\n",
    "print(max(1/weights_0))\n",
    "print(max(weights_1))\n",
    "print(max(1/weights_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_val = 3\n",
    "bins = np.linspace(0, clip_val, 101)\n",
    "plt.hist(np.clip(weights_0, 0, clip_val), bins = bins)\n",
    "plt.xlabel(\"Weight\")\n",
    "plt.ylabel('Jets per bin (normalized)')\n",
    "plt.title(\"Weights \" + label_0 + r' $\\rightarrow$ ' + label_0, fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_val = 3\n",
    "bins = np.linspace(0, clip_val, 101)\n",
    "plt.hist(np.clip(weights_1, 0, clip_val), bins = bins)\n",
    "plt.xlabel(\"Weight\")\n",
    "plt.ylabel('Jets per bin (normalized)')\n",
    "plt.title(\"Weights \" + label_0 + r' $\\rightarrow$ ' + label_1, fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define default plot styles\n",
    "plot_style_0 = {'histtype':'step', 'color':'black', 'linewidth':2, 'linestyle':'--', 'density':True}\n",
    "plot_style_1 = {'alpha':0.5, 'density':True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "bins = np.linspace(0,40,21)\n",
    "hist0 = plt.hist(test_dataset_0['multiplicity'], bins = bins, label = label_0, **plot_style_0)\n",
    "hist1 = plt.hist(test_dataset_1['multiplicity'], bins = bins, label = label_1, **plot_style_1)\n",
    "hist2 = plt.hist(test_dataset_1['multiplicity'], bins = bins, label = label_1 + ' wgt.', weights=weights_1, **plot_style_1)\n",
    "\n",
    "plt.xlabel('Multiplicity')\n",
    "plt.ylabel('Jets per bin (normalized)')\n",
    "plt.xlim([0,40])\n",
    "make_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nsubjettiness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tau21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "bins = np.linspace(0,1,31)\n",
    "hist0 = plt.hist(test_dataset_0['tau21'], bins=bins, label=label_0, **plot_style_0)\n",
    "hist1 = plt.hist(test_dataset_1['tau21'], bins=bins, label=label_1, **plot_style_1)\n",
    "hist2 = plt.hist(test_dataset_1['tau21'], bins=bins, label=label_1 +' wgt.',  weights= weights_1, **plot_style_1)\n",
    "\n",
    "plt.xlabel('tau21')\n",
    "plt.ylabel('Jets per bin (normalized)')\n",
    "make_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tau32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "bins = np.linspace(0,1,31)\n",
    "hist0 = plt.hist(test_dataset_0['tau32'], bins=bins, label=label_0, **plot_style_0)\n",
    "hist1 = plt.hist(test_dataset_1['tau32'], bins=bins, label=label_1, **plot_style_1)\n",
    "hist2 = plt.hist(test_dataset_1['tau32'], bins=bins, label=label_1 +' wgt.',  weights= weights_1, **plot_style_1)\n",
    "\n",
    "plt.xlabel('tau32')\n",
    "plt.ylabel('Jets per bin (normalized)')\n",
    "make_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ECF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N=3, $\\beta$=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "bins = np.linspace(-15,5,31)\n",
    "hist1 = plt.hist(np.log(test_dataset_0['ECF_N3_B4']), bins=bins, label=label_0, **plot_style_0)\n",
    "hist2 = plt.hist(np.log(test_dataset_1['ECF_N3_B4']), bins=bins, label=label_1, **plot_style_1)\n",
    "hist3 = plt.hist(np.log(test_dataset_1['ECF_N3_B4']), bins=bins, label=label_1 +' wgt.',  weights= weights_1, **plot_style_1)\n",
    "\n",
    "plt.xlabel('log ECF(N=3, beta=4)')\n",
    "plt.ylabel('Jets per bin (normalized)')\n",
    "make_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N=4, $\\beta$=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "bins = np.linspace(-35,5,31)\n",
    "hist1 = plt.hist(np.log(test_dataset_0['ECF_N4_B4']), bins=bins, label=label_0, **plot_style_0)\n",
    "hist2 = plt.hist(np.log(test_dataset_1['ECF_N4_B4']), bins=bins, label=label_1, **plot_style_1)\n",
    "hist3 = plt.hist(np.log(test_dataset_1['ECF_N4_B4']), bins=bins, label=label_1 +' wgt.',  weights= weights_1, **plot_style_1)\n",
    "\n",
    "plt.xlabel('log ECF(N=4, beta=4)')\n",
    "plt.ylabel('Jets per bin (normalized)')\n",
    "make_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "bins = np.linspace(0,10,11)\n",
    "hist0 = plt.hist(test_dataset_0['number_of_kaons'], bins=bins, label=label_0, **plot_style_0)\n",
    "hist1 = plt.hist(test_dataset_1['number_of_kaons'], bins=bins, label=label_1, **plot_style_1)\n",
    "hist2 = plt.hist(test_dataset_1['number_of_kaons'], bins=bins, label=label_1 +' wgt.',  weights= weights_1, **plot_style_1)\n",
    "\n",
    "plt.xlabel('Number of kaons')\n",
    "plt.ylabel('Jets per bin (normalized)')\n",
    "make_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Curve Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddParams2Input(keras.layers.Layer):\n",
    "    \"\"\" Custom layer for tuning with DCTR: \n",
    "    Arguments:\n",
    "    - n_MC_params : (int) - the number of n_MC_params that are in X_dim\n",
    "    - default_MC_params : (list of floats) - default values for each of the MC parameters\n",
    "    - trainable_MC_params : (list of booleans) - True for parameters that you want to fit, false for parameters that should be fixed at default value\n",
    "\n",
    "    Usage: \n",
    "    Let X_dim be the input dimension of each particle to a PFN model, and n_MC_params be the number of MC parameters. \n",
    "    Defines a Layer that takes in an array of dimension \n",
    "    (batch_size, padded_multiplicity, X_dim - n_MC_params)\n",
    "    This layer appends each particle by the default_MC_params and makes then trainable or non-trainable based on trainable_MC_params\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_MC_params, default_MC_params, trainable_MC_params):\n",
    "        super(AddParams2Input, self).__init__()\n",
    "        # Definitions\n",
    "        self.n_MC_params = n_MC_params\n",
    "        self.MC_params = default_MC_params\n",
    "        self.trainable_MC_params = trainable_MC_params\n",
    "\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # Convert input MC parameters to weights and make then trainable or non-trainable\n",
    "        for i in range(self.n_MC_params):\n",
    "            self.MC_params[i] = self.add_weight(name='MC_param_{}'.format(i), \n",
    "                                                shape=(1, 1),\n",
    "                                                initializer=keras.initializers.Constant(self.MC_params[i]),\n",
    "                                                trainable=self.trainable_MC_params[i])\n",
    "            \n",
    "        self.MC_params = keras.backend.tf.concat(self.MC_params, axis = -1)\n",
    "        super(AddParams2Input, self).build(input_shape)\n",
    "    \n",
    "    def call(self, input):\n",
    "        # Add MC params to each input particle (but not to the padded rows)\n",
    "        concat_input_and_params = keras.backend.tf.where(keras.backend.abs(input[...,0])>0,\n",
    "                                                         self.MC_params*keras.backend.ones_like(input[...,0:self.n_MC_params]),\n",
    "                                                         keras.backend.zeros_like(input[...,0:self.n_MC_params]))\n",
    "        return keras.backend.concatenate([input, concat_input_and_params], -1)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1]+self.n_MC_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_DCTR_fit_model(DCTR_model, \n",
    "                       X_dim, \n",
    "                       n_MC_params, \n",
    "                       default_MC_params,\n",
    "                       trainable_MC_params):\n",
    "    \"\"\" \n",
    "    Get a DCTR model that trains on the input MC parameters\n",
    "    \n",
    "    Arguments:\n",
    "    - DCTR_model : a PFN model that has been trained on a to continuously interpolate over the input MC dimensions\n",
    "    - X_dim : (int) - the dimension of the input expected by DCTR_model\n",
    "    - n_MC_params : (int) - the number of n_MC_params that are in X_dim\n",
    "    - default_MC_params : (list of floats) - default values for each of the MC parameters\n",
    "    - trainable_MC_params : (list of booleans) - True for parameters that you want to fit, false for parameters that should be fixed at default value\n",
    "\n",
    "    Returns:\n",
    "    - DCTR_fit_model: a compiled model that gradient descends only on the trainable MC parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Do sanity checks on inputs\n",
    "    assert X_dim >=n_MC_params, \"X_dim must be larger than n_MC_params. X_dim includes the dimensionality of the 4-vector + number of MC parameters\"\n",
    "    assert n_MC_params == len(default_MC_params), \"Dimension mismatch between n_MC_params and number of default MC parameters given. len(default_MC_params) must equal n_MC_params\"\n",
    "    assert n_MC_params == len(trainable_MC_params), \"Dimension mismatch between n_MC_params and trainable_MC_params. len(trainable_MC_params) must equal n_MC_params.\"\n",
    "    assert np.any(trainable_MC_params), \"All parameters are set to non-trainable.\"\n",
    "    \n",
    "    # Define input to DCTR_fit_model\n",
    "    non_param_input = keras.layers.Input((None, X_dim - n_MC_params))\n",
    "\n",
    "    # Construct layer that adds trainable and non-trainable parameters to the input\n",
    "    add_params_layer = AddParams2Input(n_MC_params, default_MC_params, trainable_MC_params)\n",
    "    time_dist     = keras.layers.TimeDistributed(add_params_layer, name='tdist')(non_param_input)     \n",
    "\n",
    "    # Set all weights in DCTR_model to non-trainable\n",
    "    for layer in DCTR_model.model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    # get the graph and the weights from the DCTR_model\n",
    "    output = DCTR_model.model(inputs = time_dist)\n",
    "\n",
    "    # Define full model\n",
    "    DCTR_fit_model = fitmodel = keras.models.Model(inputs = non_param_input, outputs = output)\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(lr=1e-4)\n",
    "    \n",
    "    # Compile with loss function\n",
    "    DCTR_fit_model.compile(optimizer=optimizer, loss='categorical_crossentropy')\n",
    "    \n",
    "    return DCTR_fit_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dctr_fit_model = get_DCTR_fit_model(dctr, \n",
    "                       X_dim =7, \n",
    "                       n_MC_params = 3, \n",
    "                       default_MC_params   = [0.1365, 0.68, 0.217], # default params for [alpha_s, aLund, StoUD]\n",
    "                       trainable_MC_params = [True, True, True]) # Only train alpha_s\n",
    "\n",
    "dctr_fit_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_MC_params(dctr_fit_model, MC_params):\n",
    "    alphaS, aLund, StoUD = MC_params\n",
    "    weights = [np.array([[alphaS]],   dtype=np.float32),\n",
    "               np.array([[aLund]],    dtype=np.float32),\n",
    "               np.array([[StoUD]], dtype=np.float32)]\n",
    "    dctr_fit_model.layers[1].set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dctr_fit_model.layers[1].get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_dataset = np.load(data_dir + 'test1D_default.npz')\n",
    "unknown_dataset = np.load(data_dir + 'test_3D_known.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_default = preprocess_data(default_dataset['jet'][:,:,:4])\n",
    "X_unknown = preprocess_data(unknown_dataset['jet'][:,:,:4])\n",
    "\n",
    "Y_default = np.zeros_like(X_unknown[:,0,0])\n",
    "Y_unknown = np.ones_like(X_unknown[:,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fit = np.concatenate((X_default, X_unknown), axis = 0)\n",
    "\n",
    "Y_fit = np.concatenate((Y_default, Y_unknown), axis = 0)\n",
    "Y_fit = to_categorical(Y_fit, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fit, _, Y_fit, _ = data_split(X_fit, Y_fit, test=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_weights = keras.callbacks.LambdaCallback(on_epoch_end=lambda batch, logs: \n",
    "                                               print(\"(alpha_s, aLund, probStoUD)=({:.4f}, {:.4f}, {:.4f})\".format(*np.array(dctr_fit_model.layers[1].get_weights()).flatten())))\n",
    "fit_vals = [[0.1365, 0.68, 0.217]]\n",
    "append_weights = keras.callbacks.LambdaCallback(on_epoch_end=lambda batch, logs: \n",
    "                                               fit_vals.append(list(np.array(dctr_fit_model.layers[1].get_weights()).flatten())))\n",
    "\n",
    "callbacks = [print_weights, append_weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dctr_fit_model.fit(X_fit, Y_fit,\n",
    "                   epochs=40, \n",
    "                   batch_size=10000,\n",
    "                   callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_vals = np.array(fit_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fit_vals[:,0]/0.1200, marker='o')\n",
    "\n",
    "plt.plot(fit_vals[:,1]/0.6000 , marker='o')\n",
    "\n",
    "plt.plot(fit_vals[:,2]/0.1200, marker='o')\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(r'fit value/target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
