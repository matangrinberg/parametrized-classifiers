{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "sys.path.append('/Users/matangrinberg/Library/CloudStorage/GoogleDrive-matan.grinberg@gmail.com/My Drive/(21-24) University of California, Berkeley/ML HEP/parametrized-classifiers/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library imports\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# standard numerical library imports\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# energyflow imports\n",
    "import energyflow as ef\n",
    "from energyflow.archs import PFN\n",
    "from energyflow.utils import data_split, remap_pids, to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Global plot settings\n",
    "# from matplotlib import rc\n",
    "# import matplotlib.font_manager\n",
    "# rc('font', family='serif')\n",
    "# rc('text', usetex=True)\n",
    "# rc('font', size=22) \n",
    "# rc('xtick', labelsize=15) \n",
    "# rc('ytick', labelsize=15) \n",
    "# rc('legend', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize pT and center (y, phi)\n",
    "def normalize(x):\n",
    "    mask = np.abs(x[:,0]) > 0\n",
    "    yphi_avg = np.average(x[mask,1:3], weights=x[mask,0], axis=0)\n",
    "    x[mask,1:3] -= yphi_avg\n",
    "    x[mask,0] /= x[:,0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X):\n",
    "    for x in X:\n",
    "        normalize(x)\n",
    "    \n",
    "    # Remap PIDs to unique values in range [0,1], error_on_unknown flag for PIDs not in dictionary\n",
    "    remap_pids(X, pid_i=3, error_on_unknown=False)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to downloaded data from Zenodo\n",
    "data_dir = '/global/home/users/mgrinberg/parametrized-classifiers/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = np.load(data_dir + '3D_train.npz')\n",
    "t_list = [1, 0.1, 0.01, 0.005, 0.001, 0.0001]\n",
    "t_string = '0.001'\n",
    "# t_string = '0.05'\n",
    "# t_string = '0.01'\n",
    "# t_string = '0.005'\n",
    "# t_string = '0.001'\n",
    "# t_string = '0.0005'\n",
    "dataset1 = np.load(data_dir + 'interpolate_standard_n100000t1.npz')\n",
    "\n",
    "dataset2 = np.load(data_dir + 'interpolate_standard_n100000t' + t_string + '.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset1['arr_0']\n",
    "Y = dataset1['arr_1']\n",
    "X2 = dataset2['arr_0']\n",
    "Y2 = dataset2['arr_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 51, 7)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = preprocess_data(X)\n",
    "Y1 = to_categorical(Y, num_classes=2)\n",
    "X2 = preprocess_data(X2)\n",
    "Y2 = to_categorical(Y2, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = data_split(X1, Y1, test=0.1, shuffle=True)\n",
    "X_train2, X_val2, Y_train2, Y_val2 = data_split(X2, Y2, test=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180000, 51, 7)\n",
      "(180000, 2)\n",
      "(20000, 51, 7)\n",
      "(20000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "print(X_val.shape)\n",
    "print(Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# network architecture parameters\n",
    "Phi_sizes = (100,100, 128)\n",
    "F_sizes = (100,100, 100)\n",
    "\n",
    "dctr = PFN(input_dim=7, Phi_sizes=Phi_sizes, F_sizes=F_sizes, summary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_label = 'DCTR_ee_dijets_3D'\n",
    "save_label = 'DCTR_test_t' + t_string\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint('./saved_models/' + save_label + '.h5', monitor='val_loss', verbose=2, save_best_only=True, mode='min')\n",
    "CSVLogger = keras.callbacks.CSVLogger('./logs/' + save_label + '_loss.csv', append=False)\n",
    "EarlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, restore_best_weights=True)\n",
    "callbacks = [checkpoint, CSVLogger, EarlyStopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "893/900 [============================>.] - ETA: 0s - loss: 1.1149 - acc: 0.5112\n",
      "Epoch 1: val_loss improved from inf to 0.70225, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 1.1116 - acc: 0.5113 - val_loss: 0.7022 - val_acc: 0.5190\n",
      "Epoch 2/100\n",
      "897/900 [============================>.] - ETA: 0s - loss: 0.7579 - acc: 0.5222\n",
      "Epoch 2: val_loss improved from 0.70225 to 0.68908, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.7576 - acc: 0.5223 - val_loss: 0.6891 - val_acc: 0.5377\n",
      "Epoch 3/100\n",
      "897/900 [============================>.] - ETA: 0s - loss: 0.7061 - acc: 0.5246\n",
      "Epoch 3: val_loss improved from 0.68908 to 0.68703, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.7060 - acc: 0.5247 - val_loss: 0.6870 - val_acc: 0.5169\n",
      "Epoch 4/100\n",
      "898/900 [============================>.] - ETA: 0s - loss: 0.6869 - acc: 0.5206\n",
      "Epoch 4: val_loss improved from 0.68703 to 0.68686, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6869 - acc: 0.5206 - val_loss: 0.6869 - val_acc: 0.5197\n",
      "Epoch 5/100\n",
      "888/900 [============================>.] - ETA: 0s - loss: 0.6864 - acc: 0.5250\n",
      "Epoch 5: val_loss improved from 0.68686 to 0.68559, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6864 - acc: 0.5250 - val_loss: 0.6856 - val_acc: 0.5294\n",
      "Epoch 6/100\n",
      "897/900 [============================>.] - ETA: 0s - loss: 0.6860 - acc: 0.5256\n",
      "Epoch 6: val_loss did not improve from 0.68559\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6860 - acc: 0.5256 - val_loss: 0.6870 - val_acc: 0.5185\n",
      "Epoch 7/100\n",
      "897/900 [============================>.] - ETA: 0s - loss: 0.6845 - acc: 0.5319\n",
      "Epoch 7: val_loss improved from 0.68559 to 0.68400, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6845 - acc: 0.5319 - val_loss: 0.6840 - val_acc: 0.5421\n",
      "Epoch 8/100\n",
      "893/900 [============================>.] - ETA: 0s - loss: 0.6857 - acc: 0.5243\n",
      "Epoch 8: val_loss did not improve from 0.68400\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6857 - acc: 0.5243 - val_loss: 0.6866 - val_acc: 0.5206\n",
      "Epoch 9/100\n",
      "895/900 [============================>.] - ETA: 0s - loss: 0.6855 - acc: 0.5238\n",
      "Epoch 9: val_loss did not improve from 0.68400\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6855 - acc: 0.5237 - val_loss: 0.6862 - val_acc: 0.5242\n",
      "Epoch 10/100\n",
      "897/900 [============================>.] - ETA: 0s - loss: 0.6856 - acc: 0.5193\n",
      "Epoch 10: val_loss did not improve from 0.68400\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6856 - acc: 0.5193 - val_loss: 0.6851 - val_acc: 0.5196\n",
      "Epoch 11/100\n",
      "897/900 [============================>.] - ETA: 0s - loss: 0.6828 - acc: 0.5348\n",
      "Epoch 11: val_loss improved from 0.68400 to 0.67859, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6828 - acc: 0.5349 - val_loss: 0.6786 - val_acc: 0.5503\n",
      "Epoch 12/100\n",
      "900/900 [==============================] - ETA: 0s - loss: 0.6833 - acc: 0.5295\n",
      "Epoch 12: val_loss improved from 0.67859 to 0.67734, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6833 - acc: 0.5295 - val_loss: 0.6773 - val_acc: 0.5470\n",
      "Epoch 13/100\n",
      "895/900 [============================>.] - ETA: 0s - loss: 0.6841 - acc: 0.5284\n",
      "Epoch 13: val_loss did not improve from 0.67734\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.6841 - acc: 0.5283 - val_loss: 0.6862 - val_acc: 0.5196\n",
      "Epoch 14/100\n",
      "896/900 [============================>.] - ETA: 0s - loss: 0.6839 - acc: 0.5237\n",
      "Epoch 14: val_loss did not improve from 0.67734\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6839 - acc: 0.5237 - val_loss: 0.6849 - val_acc: 0.5284\n",
      "Epoch 15/100\n",
      "891/900 [============================>.] - ETA: 0s - loss: 0.6801 - acc: 0.5375\n",
      "Epoch 15: val_loss improved from 0.67734 to 0.67501, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6801 - acc: 0.5376 - val_loss: 0.6750 - val_acc: 0.5492\n",
      "Epoch 16/100\n",
      "900/900 [==============================] - ETA: 0s - loss: 0.6761 - acc: 0.5469\n",
      "Epoch 16: val_loss improved from 0.67501 to 0.67371, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6761 - acc: 0.5469 - val_loss: 0.6737 - val_acc: 0.5466\n",
      "Epoch 17/100\n",
      "897/900 [============================>.] - ETA: 0s - loss: 0.6746 - acc: 0.5487\n",
      "Epoch 17: val_loss did not improve from 0.67371\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6746 - acc: 0.5486 - val_loss: 0.6761 - val_acc: 0.5474\n",
      "Epoch 18/100\n",
      "894/900 [============================>.] - ETA: 0s - loss: 0.6741 - acc: 0.5487\n",
      "Epoch 18: val_loss improved from 0.67371 to 0.67254, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.6741 - acc: 0.5486 - val_loss: 0.6725 - val_acc: 0.5475\n",
      "Epoch 19/100\n",
      "891/900 [============================>.] - ETA: 0s - loss: 0.6728 - acc: 0.5487\n",
      "Epoch 19: val_loss did not improve from 0.67254\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6728 - acc: 0.5489 - val_loss: 0.6733 - val_acc: 0.5500\n",
      "Epoch 20/100\n",
      "898/900 [============================>.] - ETA: 0s - loss: 0.6711 - acc: 0.5509\n",
      "Epoch 20: val_loss improved from 0.67254 to 0.67138, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6711 - acc: 0.5509 - val_loss: 0.6714 - val_acc: 0.5502\n",
      "Epoch 21/100\n",
      "894/900 [============================>.] - ETA: 0s - loss: 0.6702 - acc: 0.5516\n",
      "Epoch 21: val_loss did not improve from 0.67138\n",
      "900/900 [==============================] - 3s 3ms/step - loss: 0.6702 - acc: 0.5516 - val_loss: 0.6786 - val_acc: 0.5508\n",
      "Epoch 22/100\n",
      "887/900 [============================>.] - ETA: 0s - loss: 0.6695 - acc: 0.5523\n",
      "Epoch 22: val_loss improved from 0.67138 to 0.67056, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6695 - acc: 0.5525 - val_loss: 0.6706 - val_acc: 0.5534\n",
      "Epoch 23/100\n",
      "897/900 [============================>.] - ETA: 0s - loss: 0.6683 - acc: 0.5538\n",
      "Epoch 23: val_loss did not improve from 0.67056\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6683 - acc: 0.5538 - val_loss: 0.6727 - val_acc: 0.5501\n",
      "Epoch 24/100\n",
      "895/900 [============================>.] - ETA: 0s - loss: 0.6691 - acc: 0.5544\n",
      "Epoch 24: val_loss did not improve from 0.67056\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6691 - acc: 0.5544 - val_loss: 0.6745 - val_acc: 0.5503\n",
      "Epoch 25/100\n",
      "896/900 [============================>.] - ETA: 0s - loss: 0.6680 - acc: 0.5570\n",
      "Epoch 25: val_loss improved from 0.67056 to 0.66890, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6680 - acc: 0.5568 - val_loss: 0.6689 - val_acc: 0.5487\n",
      "Epoch 26/100\n",
      "891/900 [============================>.] - ETA: 0s - loss: 0.6662 - acc: 0.5577\n",
      "Epoch 26: val_loss improved from 0.66890 to 0.66837, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6662 - acc: 0.5580 - val_loss: 0.6684 - val_acc: 0.5546\n",
      "Epoch 27/100\n",
      "896/900 [============================>.] - ETA: 0s - loss: 0.6644 - acc: 0.5595\n",
      "Epoch 27: val_loss did not improve from 0.66837\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6644 - acc: 0.5593 - val_loss: 0.6687 - val_acc: 0.5547\n",
      "Epoch 28/100\n",
      "887/900 [============================>.] - ETA: 0s - loss: 0.6645 - acc: 0.5587\n",
      "Epoch 28: val_loss improved from 0.66837 to 0.66777, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6644 - acc: 0.5586 - val_loss: 0.6678 - val_acc: 0.5524\n",
      "Epoch 29/100\n",
      "897/900 [============================>.] - ETA: 0s - loss: 0.6626 - acc: 0.5608\n",
      "Epoch 29: val_loss improved from 0.66777 to 0.66485, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6626 - acc: 0.5608 - val_loss: 0.6649 - val_acc: 0.5587\n",
      "Epoch 30/100\n",
      "900/900 [==============================] - ETA: 0s - loss: 0.6616 - acc: 0.5636\n",
      "Epoch 30: val_loss did not improve from 0.66485\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.6616 - acc: 0.5636 - val_loss: 0.6662 - val_acc: 0.5532\n",
      "Epoch 31/100\n",
      "898/900 [============================>.] - ETA: 0s - loss: 0.6598 - acc: 0.5652\n",
      "Epoch 31: val_loss did not improve from 0.66485\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6598 - acc: 0.5652 - val_loss: 0.6682 - val_acc: 0.5561\n",
      "Epoch 32/100\n",
      "895/900 [============================>.] - ETA: 0s - loss: 0.6589 - acc: 0.5659\n",
      "Epoch 32: val_loss improved from 0.66485 to 0.66268, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6589 - acc: 0.5659 - val_loss: 0.6627 - val_acc: 0.5637\n",
      "Epoch 33/100\n",
      "894/900 [============================>.] - ETA: 0s - loss: 0.6578 - acc: 0.5673\n",
      "Epoch 33: val_loss did not improve from 0.66268\n",
      "900/900 [==============================] - 3s 3ms/step - loss: 0.6579 - acc: 0.5674 - val_loss: 0.6647 - val_acc: 0.5587\n",
      "Epoch 34/100\n",
      "898/900 [============================>.] - ETA: 0s - loss: 0.6585 - acc: 0.5657\n",
      "Epoch 34: val_loss did not improve from 0.66268\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6585 - acc: 0.5657 - val_loss: 0.6647 - val_acc: 0.5644\n",
      "Epoch 35/100\n",
      "887/900 [============================>.] - ETA: 0s - loss: 0.6564 - acc: 0.5686\n",
      "Epoch 35: val_loss improved from 0.66268 to 0.66182, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6564 - acc: 0.5687 - val_loss: 0.6618 - val_acc: 0.5656\n",
      "Epoch 36/100\n",
      "897/900 [============================>.] - ETA: 0s - loss: 0.6547 - acc: 0.5721\n",
      "Epoch 36: val_loss did not improve from 0.66182\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6547 - acc: 0.5719 - val_loss: 0.6622 - val_acc: 0.5652\n",
      "Epoch 37/100\n",
      "893/900 [============================>.] - ETA: 0s - loss: 0.6539 - acc: 0.5718\n",
      "Epoch 37: val_loss did not improve from 0.66182\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6539 - acc: 0.5719 - val_loss: 0.6662 - val_acc: 0.5556\n",
      "Epoch 38/100\n",
      "896/900 [============================>.] - ETA: 0s - loss: 0.6545 - acc: 0.5698\n",
      "Epoch 38: val_loss did not improve from 0.66182\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6545 - acc: 0.5697 - val_loss: 0.6620 - val_acc: 0.5648\n",
      "Epoch 39/100\n",
      "898/900 [============================>.] - ETA: 0s - loss: 0.6518 - acc: 0.5731\n",
      "Epoch 39: val_loss improved from 0.66182 to 0.66146, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6519 - acc: 0.5730 - val_loss: 0.6615 - val_acc: 0.5618\n",
      "Epoch 40/100\n",
      "895/900 [============================>.] - ETA: 0s - loss: 0.6502 - acc: 0.5778\n",
      "Epoch 40: val_loss did not improve from 0.66146\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6501 - acc: 0.5778 - val_loss: 0.6637 - val_acc: 0.5670\n",
      "Epoch 41/100\n",
      "894/900 [============================>.] - ETA: 0s - loss: 0.6492 - acc: 0.5812\n",
      "Epoch 41: val_loss improved from 0.66146 to 0.66131, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6492 - acc: 0.5812 - val_loss: 0.6613 - val_acc: 0.5718\n",
      "Epoch 42/100\n",
      "894/900 [============================>.] - ETA: 0s - loss: 0.6484 - acc: 0.5796\n",
      "Epoch 42: val_loss did not improve from 0.66131\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.6483 - acc: 0.5796 - val_loss: 0.6633 - val_acc: 0.5645\n",
      "Epoch 43/100\n",
      "897/900 [============================>.] - ETA: 0s - loss: 0.6473 - acc: 0.5799\n",
      "Epoch 43: val_loss improved from 0.66131 to 0.66121, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6473 - acc: 0.5799 - val_loss: 0.6612 - val_acc: 0.5703\n",
      "Epoch 44/100\n",
      "887/900 [============================>.] - ETA: 0s - loss: 0.6458 - acc: 0.5829\n",
      "Epoch 44: val_loss did not improve from 0.66121\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6459 - acc: 0.5830 - val_loss: 0.6622 - val_acc: 0.5683\n",
      "Epoch 45/100\n",
      "899/900 [============================>.] - ETA: 0s - loss: 0.6451 - acc: 0.5833\n",
      "Epoch 45: val_loss did not improve from 0.66121\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6451 - acc: 0.5833 - val_loss: 0.6630 - val_acc: 0.5695\n",
      "Epoch 46/100\n",
      "898/900 [============================>.] - ETA: 0s - loss: 0.6440 - acc: 0.5841\n",
      "Epoch 46: val_loss did not improve from 0.66121\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6440 - acc: 0.5841 - val_loss: 0.6631 - val_acc: 0.5699\n",
      "Epoch 47/100\n",
      "898/900 [============================>.] - ETA: 0s - loss: 0.6428 - acc: 0.5856\n",
      "Epoch 47: val_loss did not improve from 0.66121\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6428 - acc: 0.5857 - val_loss: 0.6614 - val_acc: 0.5702\n",
      "Epoch 48/100\n",
      "898/900 [============================>.] - ETA: 0s - loss: 0.6429 - acc: 0.5853\n",
      "Epoch 48: val_loss did not improve from 0.66121\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6429 - acc: 0.5853 - val_loss: 0.6636 - val_acc: 0.5726\n",
      "Epoch 49/100\n",
      "890/900 [============================>.] - ETA: 0s - loss: 0.6409 - acc: 0.5869\n",
      "Epoch 49: val_loss improved from 0.66121 to 0.66075, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6410 - acc: 0.5868 - val_loss: 0.6608 - val_acc: 0.5716\n",
      "Epoch 50/100\n",
      "900/900 [==============================] - ETA: 0s - loss: 0.6396 - acc: 0.5877\n",
      "Epoch 50: val_loss improved from 0.66075 to 0.65954, saving model to ./saved_models/DCTR_test_t0.001.h5\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6396 - acc: 0.5877 - val_loss: 0.6595 - val_acc: 0.5746\n",
      "Epoch 51/100\n",
      "899/900 [============================>.] - ETA: 0s - loss: 0.6390 - acc: 0.5894\n",
      "Epoch 51: val_loss did not improve from 0.65954\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.6389 - acc: 0.5895 - val_loss: 0.6612 - val_acc: 0.5725\n",
      "Epoch 52/100\n",
      "888/900 [============================>.] - ETA: 0s - loss: 0.6374 - acc: 0.5893\n",
      "Epoch 52: val_loss did not improve from 0.65954\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6375 - acc: 0.5893 - val_loss: 0.6614 - val_acc: 0.5675\n",
      "Epoch 53/100\n",
      "895/900 [============================>.] - ETA: 0s - loss: 0.6367 - acc: 0.5916\n",
      "Epoch 53: val_loss did not improve from 0.65954\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6368 - acc: 0.5915 - val_loss: 0.6619 - val_acc: 0.5716\n",
      "Epoch 54/100\n",
      "888/900 [============================>.] - ETA: 0s - loss: 0.6362 - acc: 0.5913\n",
      "Epoch 54: val_loss did not improve from 0.65954\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6363 - acc: 0.5913 - val_loss: 0.6700 - val_acc: 0.5731\n",
      "Epoch 55/100\n",
      "897/900 [============================>.] - ETA: 0s - loss: 0.6349 - acc: 0.5935\n",
      "Epoch 55: val_loss did not improve from 0.65954\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6350 - acc: 0.5934 - val_loss: 0.6631 - val_acc: 0.5715\n",
      "Epoch 56/100\n",
      "888/900 [============================>.] - ETA: 0s - loss: 0.6342 - acc: 0.5926\n",
      "Epoch 56: val_loss did not improve from 0.65954\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6342 - acc: 0.5926 - val_loss: 0.6638 - val_acc: 0.5724\n",
      "Epoch 57/100\n",
      "900/900 [==============================] - ETA: 0s - loss: 0.6333 - acc: 0.5922\n",
      "Epoch 57: val_loss did not improve from 0.65954\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6333 - acc: 0.5922 - val_loss: 0.6609 - val_acc: 0.5765\n",
      "Epoch 58/100\n",
      "898/900 [============================>.] - ETA: 0s - loss: 0.6320 - acc: 0.5942\n",
      "Epoch 58: val_loss did not improve from 0.65954\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6320 - acc: 0.5942 - val_loss: 0.6650 - val_acc: 0.5780\n",
      "Epoch 59/100\n",
      "897/900 [============================>.] - ETA: 0s - loss: 0.6315 - acc: 0.5958\n",
      "Epoch 59: val_loss did not improve from 0.65954\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6315 - acc: 0.5958 - val_loss: 0.6629 - val_acc: 0.5752\n",
      "Epoch 60/100\n",
      "888/900 [============================>.] - ETA: 0s - loss: 0.6305 - acc: 0.5972\n",
      "Epoch 60: val_loss did not improve from 0.65954\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "900/900 [==============================] - 4s 4ms/step - loss: 0.6305 - acc: 0.5973 - val_loss: 0.6635 - val_acc: 0.5764\n",
      "Epoch 60: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = dctr.fit(X_train2, Y_train2,\n",
    "                    epochs = 100,\n",
    "                    batch_size = 200,\n",
    "                    validation_data = (X_val, Y_val),\n",
    "                    verbose = 1, \n",
    "                    callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'],     label = 'loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val loss')\n",
    "plt.legend(loc=0)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 1s 1ms/step\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "625/625 [==============================] - 1s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# load model from saved file\n",
    "\n",
    "comp_losses = []\n",
    "for t in t_list:\n",
    "    dctr.model.load_weights('./saved_models/DCTR_test_t' + str(t) + '.h5')\n",
    "    pred = dctr.predict(X_val)\n",
    "    loss = np.sum(tf.keras.metrics.binary_crossentropy(Y_val, pred))/pred.shape[0]\n",
    "    comp_losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.635302587890625,\n",
       " 0.65094482421875,\n",
       " 0.653648779296875,\n",
       " 0.6589138671875,\n",
       " 0.659540966796875,\n",
       " 1.208744921875]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(tf.keras.metrics.binary_crossentropy(Y_val, a1))/a1.shape[0])\n",
    "print(np.sum(tf.keras.metrics.binary_crossentropy(Y_val, a2))/a2.shape[0])\n",
    "print(np.sum(tf.keras.metrics.binary_crossentropy(Y_val, a3))/a3.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([np.sum(tf.keras.metrics.binary_crossentropy(Y_val, a1))/a1.shape[0], np.sum(tf.keras.metrics.binary_crossentropy(Y_val, a2))/a2.shape[0], np.sum(tf.keras.metrics.binary_crossentropy(Y_val, a3))/a3.shape[0]])\n",
    "b = np.log(np.array([1, 0.005, 0.0001]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(t_list[0:5], comp_losses[0:5], label='loss on uniform dataset')\n",
    "plt.legend(loc=0)\n",
    "plt.ylabel('Average BCE Loss')\n",
    "plt.xlabel('t')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_0 = np.load(data_dir+'test1D_default.npz')\n",
    "test_dataset_1 = np.load(data_dir+'test_3D_known.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define labels for legends\n",
    "label_0 = r'Default'\n",
    "\n",
    "label_1 = r'Non-default'\n",
    "\n",
    "pythia_text = r'\\textsc{Pythia 8}' + '\\n' + r'$e^+e^- \\to Z \\to $ dijets' +'\\n'+ r\"anti-$k_{\\mathrm{T}}$, $R=0.8$\"\n",
    "def make_legend():\n",
    "    ax = plt.gca()\n",
    "    leg = ax.legend(frameon=False)\n",
    "    leg.set_title(pythia_text, prop={'size':14})\n",
    "    leg._legend_box.align = \"left\"\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test datasets\n",
    "X0_test = preprocess_data(test_dataset_0['jet'])\n",
    "X1_test = preprocess_data(test_dataset_1['jet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted probabilities\n",
    "preds_0 = dctr.predict(X0_test, batch_size=1000)\n",
    "preds_1 = dctr.predict(X1_test, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_0 = preds_0[:,0]/preds_0[:,1]\n",
    "weights_1 = preds_1[:,0]/preds_1[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(weights_0))\n",
    "print(max(1/weights_0))\n",
    "print(max(weights_1))\n",
    "print(max(1/weights_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_val = 3\n",
    "bins = np.linspace(0, clip_val, 101)\n",
    "plt.hist(np.clip(weights_0, 0, clip_val), bins = bins)\n",
    "plt.xlabel(\"Weight\")\n",
    "plt.ylabel('Jets per bin (normalized)')\n",
    "plt.title(\"Weights \" + label_0 + r' $\\rightarrow$ ' + label_0, fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_val = 3\n",
    "bins = np.linspace(0, clip_val, 101)\n",
    "plt.hist(np.clip(weights_1, 0, clip_val), bins = bins)\n",
    "plt.xlabel(\"Weight\")\n",
    "plt.ylabel('Jets per bin (normalized)')\n",
    "plt.title(\"Weights \" + label_0 + r' $\\rightarrow$ ' + label_1, fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define default plot styles\n",
    "plot_style_0 = {'histtype':'step', 'color':'black', 'linewidth':2, 'linestyle':'--', 'density':True}\n",
    "plot_style_1 = {'alpha':0.5, 'density':True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "bins = np.linspace(0,40,21)\n",
    "hist0 = plt.hist(test_dataset_0['multiplicity'], bins = bins, label = label_0, **plot_style_0)\n",
    "hist1 = plt.hist(test_dataset_1['multiplicity'], bins = bins, label = label_1, **plot_style_1)\n",
    "hist2 = plt.hist(test_dataset_1['multiplicity'], bins = bins, label = label_1 + ' wgt.', weights=weights_1, **plot_style_1)\n",
    "\n",
    "plt.xlabel('Multiplicity')\n",
    "plt.ylabel('Jets per bin (normalized)')\n",
    "plt.xlim([0,40])\n",
    "make_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nsubjettiness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tau21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "bins = np.linspace(0,1,31)\n",
    "hist0 = plt.hist(test_dataset_0['tau21'], bins=bins, label=label_0, **plot_style_0)\n",
    "hist1 = plt.hist(test_dataset_1['tau21'], bins=bins, label=label_1, **plot_style_1)\n",
    "hist2 = plt.hist(test_dataset_1['tau21'], bins=bins, label=label_1 +' wgt.',  weights= weights_1, **plot_style_1)\n",
    "\n",
    "plt.xlabel('tau21')\n",
    "plt.ylabel('Jets per bin (normalized)')\n",
    "make_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tau32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "bins = np.linspace(0,1,31)\n",
    "hist0 = plt.hist(test_dataset_0['tau32'], bins=bins, label=label_0, **plot_style_0)\n",
    "hist1 = plt.hist(test_dataset_1['tau32'], bins=bins, label=label_1, **plot_style_1)\n",
    "hist2 = plt.hist(test_dataset_1['tau32'], bins=bins, label=label_1 +' wgt.',  weights= weights_1, **plot_style_1)\n",
    "\n",
    "plt.xlabel('tau32')\n",
    "plt.ylabel('Jets per bin (normalized)')\n",
    "make_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ECF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N=3, $\\beta$=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "bins = np.linspace(-15,5,31)\n",
    "hist1 = plt.hist(np.log(test_dataset_0['ECF_N3_B4']), bins=bins, label=label_0, **plot_style_0)\n",
    "hist2 = plt.hist(np.log(test_dataset_1['ECF_N3_B4']), bins=bins, label=label_1, **plot_style_1)\n",
    "hist3 = plt.hist(np.log(test_dataset_1['ECF_N3_B4']), bins=bins, label=label_1 +' wgt.',  weights= weights_1, **plot_style_1)\n",
    "\n",
    "plt.xlabel('log ECF(N=3, beta=4)')\n",
    "plt.ylabel('Jets per bin (normalized)')\n",
    "make_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N=4, $\\beta$=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "bins = np.linspace(-35,5,31)\n",
    "hist1 = plt.hist(np.log(test_dataset_0['ECF_N4_B4']), bins=bins, label=label_0, **plot_style_0)\n",
    "hist2 = plt.hist(np.log(test_dataset_1['ECF_N4_B4']), bins=bins, label=label_1, **plot_style_1)\n",
    "hist3 = plt.hist(np.log(test_dataset_1['ECF_N4_B4']), bins=bins, label=label_1 +' wgt.',  weights= weights_1, **plot_style_1)\n",
    "\n",
    "plt.xlabel('log ECF(N=4, beta=4)')\n",
    "plt.ylabel('Jets per bin (normalized)')\n",
    "make_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "bins = np.linspace(0,10,11)\n",
    "hist0 = plt.hist(test_dataset_0['number_of_kaons'], bins=bins, label=label_0, **plot_style_0)\n",
    "hist1 = plt.hist(test_dataset_1['number_of_kaons'], bins=bins, label=label_1, **plot_style_1)\n",
    "hist2 = plt.hist(test_dataset_1['number_of_kaons'], bins=bins, label=label_1 +' wgt.',  weights= weights_1, **plot_style_1)\n",
    "\n",
    "plt.xlabel('Number of kaons')\n",
    "plt.ylabel('Jets per bin (normalized)')\n",
    "make_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Curve Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddParams2Input(keras.layers.Layer):\n",
    "    \"\"\" Custom layer for tuning with DCTR: \n",
    "    Arguments:\n",
    "    - n_MC_params : (int) - the number of n_MC_params that are in X_dim\n",
    "    - default_MC_params : (list of floats) - default values for each of the MC parameters\n",
    "    - trainable_MC_params : (list of booleans) - True for parameters that you want to fit, false for parameters that should be fixed at default value\n",
    "\n",
    "    Usage: \n",
    "    Let X_dim be the input dimension of each particle to a PFN model, and n_MC_params be the number of MC parameters. \n",
    "    Defines a Layer that takes in an array of dimension \n",
    "    (batch_size, padded_multiplicity, X_dim - n_MC_params)\n",
    "    This layer appends each particle by the default_MC_params and makes then trainable or non-trainable based on trainable_MC_params\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_MC_params, default_MC_params, trainable_MC_params):\n",
    "        super(AddParams2Input, self).__init__()\n",
    "        # Definitions\n",
    "        self.n_MC_params = n_MC_params\n",
    "        self.MC_params = default_MC_params\n",
    "        self.trainable_MC_params = trainable_MC_params\n",
    "\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # Convert input MC parameters to weights and make then trainable or non-trainable\n",
    "        for i in range(self.n_MC_params):\n",
    "            self.MC_params[i] = self.add_weight(name='MC_param_{}'.format(i), \n",
    "                                                shape=(1, 1),\n",
    "                                                initializer=keras.initializers.Constant(self.MC_params[i]),\n",
    "                                                trainable=self.trainable_MC_params[i])\n",
    "            \n",
    "        self.MC_params = keras.backend.tf.concat(self.MC_params, axis = -1)\n",
    "        super(AddParams2Input, self).build(input_shape)\n",
    "    \n",
    "    def call(self, input):\n",
    "        # Add MC params to each input particle (but not to the padded rows)\n",
    "        concat_input_and_params = keras.backend.tf.where(keras.backend.abs(input[...,0])>0,\n",
    "                                                         self.MC_params*keras.backend.ones_like(input[...,0:self.n_MC_params]),\n",
    "                                                         keras.backend.zeros_like(input[...,0:self.n_MC_params]))\n",
    "        return keras.backend.concatenate([input, concat_input_and_params], -1)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1]+self.n_MC_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_DCTR_fit_model(DCTR_model, \n",
    "                       X_dim, \n",
    "                       n_MC_params, \n",
    "                       default_MC_params,\n",
    "                       trainable_MC_params):\n",
    "    \"\"\" \n",
    "    Get a DCTR model that trains on the input MC parameters\n",
    "    \n",
    "    Arguments:\n",
    "    - DCTR_model : a PFN model that has been trained on a to continuously interpolate over the input MC dimensions\n",
    "    - X_dim : (int) - the dimension of the input expected by DCTR_model\n",
    "    - n_MC_params : (int) - the number of n_MC_params that are in X_dim\n",
    "    - default_MC_params : (list of floats) - default values for each of the MC parameters\n",
    "    - trainable_MC_params : (list of booleans) - True for parameters that you want to fit, false for parameters that should be fixed at default value\n",
    "\n",
    "    Returns:\n",
    "    - DCTR_fit_model: a compiled model that gradient descends only on the trainable MC parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Do sanity checks on inputs\n",
    "    assert X_dim >=n_MC_params, \"X_dim must be larger than n_MC_params. X_dim includes the dimensionality of the 4-vector + number of MC parameters\"\n",
    "    assert n_MC_params == len(default_MC_params), \"Dimension mismatch between n_MC_params and number of default MC parameters given. len(default_MC_params) must equal n_MC_params\"\n",
    "    assert n_MC_params == len(trainable_MC_params), \"Dimension mismatch between n_MC_params and trainable_MC_params. len(trainable_MC_params) must equal n_MC_params.\"\n",
    "    assert np.any(trainable_MC_params), \"All parameters are set to non-trainable.\"\n",
    "    \n",
    "    # Define input to DCTR_fit_model\n",
    "    non_param_input = keras.layers.Input((None, X_dim - n_MC_params))\n",
    "\n",
    "    # Construct layer that adds trainable and non-trainable parameters to the input\n",
    "    add_params_layer = AddParams2Input(n_MC_params, default_MC_params, trainable_MC_params)\n",
    "    time_dist     = keras.layers.TimeDistributed(add_params_layer, name='tdist')(non_param_input)     \n",
    "\n",
    "    # Set all weights in DCTR_model to non-trainable\n",
    "    for layer in DCTR_model.model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    # get the graph and the weights from the DCTR_model\n",
    "    output = DCTR_model.model(inputs = time_dist)\n",
    "\n",
    "    # Define full model\n",
    "    DCTR_fit_model = fitmodel = keras.models.Model(inputs = non_param_input, outputs = output)\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(lr=1e-4)\n",
    "    \n",
    "    # Compile with loss function\n",
    "    DCTR_fit_model.compile(optimizer=optimizer, loss='categorical_crossentropy')\n",
    "    \n",
    "    return DCTR_fit_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dctr_fit_model = get_DCTR_fit_model(dctr, \n",
    "                       X_dim =7, \n",
    "                       n_MC_params = 3, \n",
    "                       default_MC_params   = [0.1365, 0.68, 0.217], # default params for [alpha_s, aLund, StoUD]\n",
    "                       trainable_MC_params = [True, True, True]) # Only train alpha_s\n",
    "\n",
    "dctr_fit_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_MC_params(dctr_fit_model, MC_params):\n",
    "    alphaS, aLund, StoUD = MC_params\n",
    "    weights = [np.array([[alphaS]],   dtype=np.float32),\n",
    "               np.array([[aLund]],    dtype=np.float32),\n",
    "               np.array([[StoUD]], dtype=np.float32)]\n",
    "    dctr_fit_model.layers[1].set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dctr_fit_model.layers[1].get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_dataset = np.load(data_dir + 'test1D_default.npz')\n",
    "unknown_dataset = np.load(data_dir + 'test_3D_known.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_default = preprocess_data(default_dataset['jet'][:,:,:4])\n",
    "X_unknown = preprocess_data(unknown_dataset['jet'][:,:,:4])\n",
    "\n",
    "Y_default = np.zeros_like(X_unknown[:,0,0])\n",
    "Y_unknown = np.ones_like(X_unknown[:,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fit = np.concatenate((X_default, X_unknown), axis = 0)\n",
    "\n",
    "Y_fit = np.concatenate((Y_default, Y_unknown), axis = 0)\n",
    "Y_fit = to_categorical(Y_fit, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fit, _, Y_fit, _ = data_split(X_fit, Y_fit, test=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_weights = keras.callbacks.LambdaCallback(on_epoch_end=lambda batch, logs: \n",
    "                                               print(\"(alpha_s, aLund, probStoUD)=({:.4f}, {:.4f}, {:.4f})\".format(*np.array(dctr_fit_model.layers[1].get_weights()).flatten())))\n",
    "fit_vals = [[0.1365, 0.68, 0.217]]\n",
    "append_weights = keras.callbacks.LambdaCallback(on_epoch_end=lambda batch, logs: \n",
    "                                               fit_vals.append(list(np.array(dctr_fit_model.layers[1].get_weights()).flatten())))\n",
    "\n",
    "callbacks = [print_weights, append_weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dctr_fit_model.fit(X_fit, Y_fit,\n",
    "                   epochs=40, \n",
    "                   batch_size=10000,\n",
    "                   callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_vals = np.array(fit_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fit_vals[:,0]/0.1200, marker='o')\n",
    "\n",
    "plt.plot(fit_vals[:,1]/0.6000 , marker='o')\n",
    "\n",
    "plt.plot(fit_vals[:,2]/0.1200, marker='o')\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(r'fit value/target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
